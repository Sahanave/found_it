{
    "id": "7386195f-2eff-4353-be3d-eb1f78f2124a",
    "data": {
        "nodes": [
            {
                "data": {
                    "description": "Get chat inputs from the Playground.",
                    "display_name": "Chat Input",
                    "id": "ChatInput-0PoBn",
                    "node": {
                        "base_classes": [
                            "Message"
                        ],
                        "beta": false,
                        "conditional_paths": [],
                        "custom_fields": {},
                        "description": "Get chat inputs from the Playground.",
                        "display_name": "Chat Input",
                        "documentation": "",
                        "edited": false,
                        "field_order": [
                            "input_value",
                            "should_store_message",
                            "sender",
                            "sender_name",
                            "session_id",
                            "files"
                        ],
                        "frozen": false,
                        "icon": "ChatInput",
                        "metadata": {},
                        "output_types": [],
                        "outputs": [
                            {
                                "cache": true,
                                "display_name": "Message",
                                "method": "message_response",
                                "name": "message",
                                "selected": "Message",
                                "types": [
                                    "Message"
                                ],
                                "value": "__UNDEFINED__"
                            }
                        ],
                        "pinned": false,
                        "template": {
                            "_type": "Component",
                            "code": {
                                "advanced": true,
                                "dynamic": true,
                                "fileTypes": [],
                                "file_path": "",
                                "info": "",
                                "list": false,
                                "load_from_db": false,
                                "multiline": true,
                                "name": "code",
                                "password": false,
                                "placeholder": "",
                                "required": true,
                                "show": true,
                                "title_case": false,
                                "type": "code",
                                "value": "from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_USER, MESSAGE_SENDER_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n"
                            },
                            "files": {
                                "advanced": true,
                                "display_name": "Files",
                                "dynamic": false,
                                "fileTypes": [
                                    "txt",
                                    "md",
                                    "mdx",
                                    "csv",
                                    "json",
                                    "yaml",
                                    "yml",
                                    "xml",
                                    "html",
                                    "htm",
                                    "pdf",
                                    "docx",
                                    "py",
                                    "sh",
                                    "sql",
                                    "js",
                                    "ts",
                                    "tsx",
                                    "jpg",
                                    "jpeg",
                                    "png",
                                    "bmp",
                                    "image"
                                ],
                                "file_path": "",
                                "info": "Files to be sent with the message.",
                                "list": true,
                                "name": "files",
                                "placeholder": "",
                                "required": false,
                                "show": true,
                                "title_case": false,
                                "trace_as_metadata": true,
                                "type": "file",
                                "value": ""
                            },
                            "input_value": {
                                "advanced": false,
                                "display_name": "Text",
                                "dynamic": false,
                                "info": "Message to be passed as input.",
                                "input_types": [
                                    "Message"
                                ],
                                "list": false,
                                "load_from_db": false,
                                "multiline": true,
                                "name": "input_value",
                                "placeholder": "",
                                "required": false,
                                "show": true,
                                "title_case": false,
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "type": "str",
                                "value": "tell me why a lot of Ai startups fail, whenever they are asked for the ROI"
                            },
                            "sender": {
                                "advanced": true,
                                "display_name": "Sender Type",
                                "dynamic": false,
                                "info": "Type of sender.",
                                "name": "sender",
                                "options": [
                                    "Machine",
                                    "User"
                                ],
                                "placeholder": "",
                                "required": false,
                                "show": true,
                                "title_case": false,
                                "trace_as_metadata": true,
                                "type": "str",
                                "value": "User"
                            },
                            "sender_name": {
                                "advanced": true,
                                "display_name": "Sender Name",
                                "dynamic": false,
                                "info": "Name of the sender.",
                                "input_types": [
                                    "Message"
                                ],
                                "list": false,
                                "load_from_db": false,
                                "name": "sender_name",
                                "placeholder": "",
                                "required": false,
                                "show": true,
                                "title_case": false,
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "type": "str",
                                "value": "User"
                            },
                            "session_id": {
                                "advanced": true,
                                "display_name": "Session ID",
                                "dynamic": false,
                                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                                "input_types": [
                                    "Message"
                                ],
                                "list": false,
                                "load_from_db": false,
                                "name": "session_id",
                                "placeholder": "",
                                "required": false,
                                "show": true,
                                "title_case": false,
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "type": "str",
                                "value": ""
                            },
                            "should_store_message": {
                                "advanced": true,
                                "display_name": "Store Messages",
                                "dynamic": false,
                                "info": "Store the message in the history.",
                                "list": false,
                                "name": "should_store_message",
                                "placeholder": "",
                                "required": false,
                                "show": true,
                                "title_case": false,
                                "trace_as_metadata": true,
                                "type": "bool",
                                "value": true
                            }
                        },
                        "lf_version": "1.0.19.post2"
                    },
                    "type": "ChatInput"
                },
                "dragging": false,
                "height": 289,
                "id": "ChatInput-0PoBn",
                "position": {
                    "x": 276.75768295947455,
                    "y": -259.4534412002621
                },
                "positionAbsolute": {
                    "x": 276.75768295947455,
                    "y": -259.4534412002621
                },
                "selected": false,
                "type": "genericNode",
                "width": 384
            },
            {
                "data": {
                    "description": "Convert Data into plain text following a specified template.",
                    "display_name": "Parse Data",
                    "id": "ParseData-qNt15",
                    "node": {
                        "base_classes": [
                            "Message"
                        ],
                        "beta": false,
                        "conditional_paths": [],
                        "custom_fields": {},
                        "description": "Convert Data into plain text following a specified template.",
                        "display_name": "Parse Data",
                        "documentation": "",
                        "edited": false,
                        "field_order": [
                            "data",
                            "template",
                            "sep"
                        ],
                        "frozen": false,
                        "icon": "braces",
                        "metadata": {},
                        "output_types": [],
                        "outputs": [
                            {
                                "cache": true,
                                "display_name": "Text",
                                "method": "parse_data",
                                "name": "text",
                                "selected": "Message",
                                "types": [
                                    "Message"
                                ],
                                "value": "__UNDEFINED__"
                            }
                        ],
                        "pinned": false,
                        "template": {
                            "_type": "Component",
                            "code": {
                                "advanced": true,
                                "dynamic": true,
                                "fileTypes": [],
                                "file_path": "",
                                "info": "",
                                "list": false,
                                "load_from_db": false,
                                "multiline": true,
                                "name": "code",
                                "password": false,
                                "placeholder": "",
                                "required": true,
                                "show": true,
                                "title_case": false,
                                "type": "code",
                                "value": "from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. \"\n            \"It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n"
                            },
                            "data": {
                                "advanced": false,
                                "display_name": "Data",
                                "dynamic": false,
                                "info": "The data to convert to text.",
                                "input_types": [
                                    "Data"
                                ],
                                "list": false,
                                "name": "data",
                                "placeholder": "",
                                "required": false,
                                "show": true,
                                "title_case": false,
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "type": "other",
                                "value": ""
                            },
                            "sep": {
                                "advanced": true,
                                "display_name": "Separator",
                                "dynamic": false,
                                "info": "",
                                "list": false,
                                "load_from_db": false,
                                "name": "sep",
                                "placeholder": "",
                                "required": false,
                                "show": true,
                                "title_case": false,
                                "trace_as_metadata": true,
                                "type": "str",
                                "value": "\n"
                            },
                            "template": {
                                "advanced": false,
                                "display_name": "Template",
                                "dynamic": false,
                                "info": "The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.",
                                "input_types": [
                                    "Message"
                                ],
                                "list": false,
                                "load_from_db": false,
                                "multiline": true,
                                "name": "template",
                                "placeholder": "",
                                "required": false,
                                "show": true,
                                "title_case": false,
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "type": "str",
                                "value": "<document>\n{text}\n</document>"
                            }
                        },
                        "lf_version": "1.0.19.post2"
                    },
                    "type": "ParseData"
                },
                "dragging": false,
                "height": 353,
                "id": "ParseData-qNt15",
                "position": {
                    "x": 2003.948823553551,
                    "y": 415.43812691964945
                },
                "positionAbsolute": {
                    "x": 2003.948823553551,
                    "y": 415.43812691964945
                },
                "selected": false,
                "type": "genericNode",
                "width": 384
            },
            {
                "data": {
                    "description": "Create a prompt template with dynamic variables.",
                    "display_name": "Prompt",
                    "id": "Prompt-KEwey",
                    "node": {
                        "template": {
                            "_type": "Component",
                            "code": {
                                "advanced": true,
                                "dynamic": true,
                                "fileTypes": [],
                                "file_path": "",
                                "info": "",
                                "list": false,
                                "load_from_db": false,
                                "multiline": true,
                                "name": "code",
                                "password": false,
                                "placeholder": "",
                                "required": true,
                                "show": true,
                                "title_case": false,
                                "type": "code",
                                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
                            },
                            "context": {
                                "field_type": "str",
                                "required": false,
                                "placeholder": "",
                                "list": false,
                                "show": true,
                                "multiline": true,
                                "value": "",
                                "fileTypes": [],
                                "file_path": "",
                                "name": "context",
                                "display_name": "context",
                                "advanced": false,
                                "input_types": [
                                    "Message",
                                    "Text"
                                ],
                                "dynamic": false,
                                "info": "",
                                "load_from_db": false,
                                "title_case": false,
                                "type": "str"
                            },
                            "question": {
                                "field_type": "str",
                                "required": false,
                                "placeholder": "",
                                "list": false,
                                "show": true,
                                "multiline": true,
                                "value": "",
                                "fileTypes": [],
                                "file_path": "",
                                "name": "question",
                                "display_name": "question",
                                "advanced": false,
                                "input_types": [
                                    "Message",
                                    "Text"
                                ],
                                "dynamic": false,
                                "info": "",
                                "load_from_db": false,
                                "title_case": false,
                                "type": "str"
                            },
                            "template": {
                                "advanced": false,
                                "display_name": "Template",
                                "dynamic": false,
                                "info": "",
                                "list": false,
                                "load_from_db": false,
                                "name": "template",
                                "placeholder": "",
                                "required": false,
                                "show": true,
                                "title_case": false,
                                "trace_as_input": true,
                                "type": "prompt",
                                "value": "## Internal Context\n{context}\n\n## Insights from the internet\n{insights}\n\n## Outline\n{outline}\n\n## Instruction\nGiven the context above and context , answer the question as best as possible.\n\nQuestion: \n```\n{question}\n```"
                            },
                            "insights": {
                                "field_type": "str",
                                "required": false,
                                "placeholder": "",
                                "list": false,
                                "show": true,
                                "multiline": true,
                                "value": "",
                                "fileTypes": [],
                                "file_path": "",
                                "name": "insights",
                                "display_name": "insights",
                                "advanced": false,
                                "input_types": [
                                    "Message",
                                    "Text"
                                ],
                                "dynamic": false,
                                "info": "",
                                "load_from_db": false,
                                "title_case": false,
                                "type": "str"
                            },
                            "outline": {
                                "field_type": "str",
                                "required": false,
                                "placeholder": "",
                                "list": false,
                                "show": true,
                                "multiline": true,
                                "value": "",
                                "fileTypes": [],
                                "file_path": "",
                                "name": "outline",
                                "display_name": "outline",
                                "advanced": false,
                                "input_types": [
                                    "Message",
                                    "Text"
                                ],
                                "dynamic": false,
                                "info": "",
                                "load_from_db": false,
                                "title_case": false,
                                "type": "str"
                            }
                        },
                        "description": "Create a prompt template with dynamic variables.",
                        "icon": "prompts",
                        "is_input": null,
                        "is_output": null,
                        "is_composition": null,
                        "base_classes": [
                            "Message"
                        ],
                        "name": "",
                        "display_name": "Prompt",
                        "documentation": "",
                        "custom_fields": {
                            "template": [
                                "context",
                                "insights",
                                "outline",
                                "question"
                            ]
                        },
                        "output_types": [],
                        "full_path": null,
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                            {
                                "types": [
                                    "Message"
                                ],
                                "selected": "Message",
                                "name": "prompt",
                                "hidden": null,
                                "display_name": "Prompt Message",
                                "method": "build_prompt",
                                "value": "__UNDEFINED__",
                                "cache": true,
                                "required_inputs": null
                            }
                        ],
                        "field_order": [
                            "template"
                        ],
                        "beta": false,
                        "error": null,
                        "edited": false,
                        "metadata": {},
                        "lf_version": "1.0.19.post2"
                    },
                    "type": "Prompt"
                },
                "dragging": false,
                "height": 649,
                "id": "Prompt-KEwey",
                "position": {
                    "x": 3811.854931794044,
                    "y": 1086.2362766402885
                },
                "positionAbsolute": {
                    "x": 3811.854931794044,
                    "y": 1086.2362766402885
                },
                "selected": false,
                "type": "genericNode",
                "width": 384
            },
            {
                "data": {
                    "description": "Display a chat message in the Playground. \n💡 Click the ▶️ to run the flow",
                    "display_name": "Chat Output",
                    "id": "ChatOutput-VDBdC",
                    "node": {
                        "base_classes": [
                            "Message"
                        ],
                        "beta": false,
                        "conditional_paths": [],
                        "custom_fields": {},
                        "description": "Display a chat message in the Playground. \n💡 Click the ▶️ to run the flow",
                        "display_name": "Chat Output",
                        "documentation": "",
                        "edited": false,
                        "field_order": [
                            "input_value",
                            "should_store_message",
                            "sender",
                            "sender_name",
                            "session_id",
                            "data_template"
                        ],
                        "frozen": false,
                        "icon": "ChatOutput",
                        "metadata": {},
                        "output_types": [],
                        "outputs": [
                            {
                                "cache": true,
                                "display_name": "Message",
                                "method": "message_response",
                                "name": "message",
                                "selected": "Message",
                                "types": [
                                    "Message"
                                ],
                                "value": "__UNDEFINED__"
                            }
                        ],
                        "pinned": false,
                        "template": {
                            "_type": "Component",
                            "code": {
                                "advanced": true,
                                "dynamic": true,
                                "fileTypes": [],
                                "file_path": "",
                                "info": "",
                                "list": false,
                                "load_from_db": false,
                                "multiline": true,
                                "name": "code",
                                "password": false,
                                "placeholder": "",
                                "required": true,
                                "show": true,
                                "title_case": false,
                                "type": "code",
                                "value": "from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n"
                            },
                            "data_template": {
                                "advanced": true,
                                "display_name": "Data Template",
                                "dynamic": false,
                                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                                "input_types": [
                                    "Message"
                                ],
                                "list": false,
                                "load_from_db": false,
                                "name": "data_template",
                                "placeholder": "",
                                "required": false,
                                "show": true,
                                "title_case": false,
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "type": "str",
                                "value": "{text}"
                            },
                            "input_value": {
                                "advanced": false,
                                "display_name": "Text",
                                "dynamic": false,
                                "info": "Message to be passed as output.",
                                "input_types": [
                                    "Message"
                                ],
                                "list": false,
                                "load_from_db": false,
                                "name": "input_value",
                                "placeholder": "",
                                "required": false,
                                "show": true,
                                "title_case": false,
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "type": "str",
                                "value": ""
                            },
                            "sender": {
                                "advanced": true,
                                "display_name": "Sender Type",
                                "dynamic": false,
                                "info": "Type of sender.",
                                "name": "sender",
                                "options": [
                                    "Machine",
                                    "User"
                                ],
                                "placeholder": "",
                                "required": false,
                                "show": true,
                                "title_case": false,
                                "trace_as_metadata": true,
                                "type": "str",
                                "value": "Machine"
                            },
                            "sender_name": {
                                "advanced": true,
                                "display_name": "Sender Name",
                                "dynamic": false,
                                "info": "Name of the sender.",
                                "input_types": [
                                    "Message"
                                ],
                                "list": false,
                                "load_from_db": false,
                                "name": "sender_name",
                                "placeholder": "",
                                "required": false,
                                "show": true,
                                "title_case": false,
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "type": "str",
                                "value": "AI"
                            },
                            "session_id": {
                                "advanced": true,
                                "display_name": "Session ID",
                                "dynamic": false,
                                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                                "input_types": [
                                    "Message"
                                ],
                                "list": false,
                                "load_from_db": false,
                                "name": "session_id",
                                "placeholder": "",
                                "required": false,
                                "show": true,
                                "title_case": false,
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "type": "str",
                                "value": ""
                            },
                            "should_store_message": {
                                "advanced": true,
                                "display_name": "Store Messages",
                                "dynamic": false,
                                "info": "Store the message in the history.",
                                "list": false,
                                "name": "should_store_message",
                                "placeholder": "",
                                "required": false,
                                "show": true,
                                "title_case": false,
                                "trace_as_metadata": true,
                                "type": "bool",
                                "value": true
                            }
                        },
                        "lf_version": "1.0.19.post2"
                    },
                    "type": "ChatOutput"
                },
                "dragging": false,
                "height": 305,
                "id": "ChatOutput-VDBdC",
                "position": {
                    "x": 5489.726673198236,
                    "y": 1049.0234300951538
                },
                "positionAbsolute": {
                    "x": 5489.726673198236,
                    "y": 1049.0234300951538
                },
                "selected": false,
                "type": "genericNode",
                "width": 384
            },
            {
                "id": "MultiQueryRetriever-DEUr2",
                "type": "genericNode",
                "position": {
                    "x": 1535.1653510512647,
                    "y": 347.8162545071665
                },
                "data": {
                    "type": "MultiQueryRetriever",
                    "node": {
                        "template": {
                            "_type": "Component",
                            "llm": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "llm",
                                "value": "",
                                "display_name": "LLM",
                                "advanced": false,
                                "input_types": [
                                    "LanguageModel"
                                ],
                                "dynamic": false,
                                "info": "LLM to be passed as input.",
                                "title_case": false,
                                "type": "other",
                                "_input_type": "HandleInput"
                            },
                            "retriever": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "retriever",
                                "value": "",
                                "display_name": "Retriever",
                                "advanced": false,
                                "input_types": [
                                    "Retriever"
                                ],
                                "dynamic": false,
                                "info": "",
                                "title_case": false,
                                "type": "other",
                                "_input_type": "HandleInput"
                            },
                            "code": {
                                "type": "code",
                                "required": true,
                                "placeholder": "",
                                "list": false,
                                "show": true,
                                "multiline": true,
                                "value": "from typing import Optional, List\n\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\n\nfrom langflow.custom import Component\n\nfrom langflow.inputs import HandleInput, MessageTextInput, MultilineInput\n\nfrom langflow.io import Output\nfrom langflow.schema import Data\nfrom langflow.helpers.data import docs_to_data\n\n\nfrom langchain_core.output_parsers import BaseOutputParser\nfrom pydantic import BaseModel, Field\nfrom langchain_core.prompts import PromptTemplate\n\n\n\nclass MultiQueryRetrieverComponent(Component):\n    display_name = \"MultiQueryRetriever\"\n    description = \"Initialize from llm using default template.\"\n    documentation = \"https://python.langchain.com/docs/how_to/MultiQueryRetriever/\"\n    name = \"MultiQueryRetriever\"\n    \n    inputs = [\n        MultilineInput(\n            name=\"search_query\",\n            display_name=\"Query\",\n            info=\"Query to be passed as input.\",\n            input_types=[\"Message\", \"Text\"],\n        ),\n        HandleInput(\n            name=\"llm\",\n            display_name=\"LLM\",\n            info=\"LLM to be passed as input.\",\n            input_types=[\"LanguageModel\"],\n        ),\n        HandleInput(name=\"retriever\", display_name=\"Retriever\", input_types=[\"Retriever\"]),\n    ]\n    \n    outputs = [\n        Output(display_name=\"Retrieved Documents\", name=\"documents\", method=\"retrieve_documents\"),\n    ]\n    \n\n    def retrieve_documents(self) -> List[Data]:\n        QUERY_PROMPT = PromptTemplate(\n            input_variables=[\"question\"],\n            template=\"\"\"You are an AI language model assistant. Your task is to generate five \n            different versions of the given user question to retrieve relevant documents from a vector \n            database. By generating multiple perspectives on the user question, your goal is to help\n            the user overcome some of the limitations of the distance-based similarity search. \n            Provide these alternative questions separated by newlines.\n            Original question: {question}\"\"\",\n        )\n        retriever = MultiQueryRetriever.from_llm(\n            retriever=self.retriever, llm=self.llm, prompt=QUERY_PROMPT, include_original=True, parser_key=\"lines\"\n        )  # \"lines\" is the key (attribute name) of the parsed output\n        \n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\n            unique_docs = retriever.invoke(self.search_query)\n            data = docs_to_data(unique_docs)\n            self.status = data\n            \n            return data\n            \n        else:\n            return []\n    \n\n",
                                "fileTypes": [],
                                "file_path": "",
                                "password": false,
                                "name": "code",
                                "advanced": true,
                                "dynamic": true,
                                "info": "",
                                "load_from_db": false,
                                "title_case": false
                            },
                            "search_query": {
                                "trace_as_input": true,
                                "multiline": true,
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "search_query",
                                "value": "",
                                "display_name": "Query",
                                "advanced": false,
                                "input_types": [
                                    "Message",
                                    "Text"
                                ],
                                "dynamic": false,
                                "info": "Query to be passed as input.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "MultilineInput"
                            }
                        },
                        "description": "Initialize from llm using default template.",
                        "base_classes": [
                            "Data"
                        ],
                        "display_name": "MultiQueryRetriever",
                        "documentation": "https://python.langchain.com/docs/how_to/MultiQueryRetriever/",
                        "custom_fields": {},
                        "output_types": [],
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                            {
                                "types": [
                                    "Data"
                                ],
                                "selected": "Data",
                                "name": "documents",
                                "display_name": "Retrieved Documents",
                                "method": "retrieve_documents",
                                "value": "__UNDEFINED__",
                                "cache": true
                            }
                        ],
                        "field_order": [
                            "search_query",
                            "llm",
                            "retriever"
                        ],
                        "beta": false,
                        "edited": true,
                        "official": false,
                        "lf_version": "1.0.19.post2"
                    },
                    "id": "MultiQueryRetriever-DEUr2"
                },
                "selected": false,
                "width": 384,
                "height": 383,
                "positionAbsolute": {
                    "x": 1535.1653510512647,
                    "y": 347.8162545071665
                },
                "dragging": false
            },
            {
                "id": "Prompt-k5TEn",
                "type": "genericNode",
                "position": {
                    "x": 4910.103348280599,
                    "y": 123.2280910726034
                },
                "data": {
                    "type": "Prompt",
                    "node": {
                        "template": {
                            "_type": "Component",
                            "code": {
                                "type": "code",
                                "required": true,
                                "placeholder": "",
                                "list": false,
                                "show": true,
                                "multiline": true,
                                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                                "fileTypes": [],
                                "file_path": "",
                                "password": false,
                                "name": "code",
                                "advanced": true,
                                "dynamic": true,
                                "info": "",
                                "load_from_db": false,
                                "title_case": false
                            },
                            "template": {
                                "trace_as_input": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "template",
                                "value": "Please act as an impartial judge and evaluate the quality of the provided answer which attempts to answer the provided question based on a provided context.\nYou'll be given a function grading_function which you'll call for each provided context, question and answer to submit your reasoning and score for the correctness, comprehensiveness and readability of the answer. \n  \n## Grading rubric\nBelow is your grading rubric: \n1. Correctness: If the answer correctly answer the question, below are the details for different scores:\n  - Score 0: the answer is completely incorrect, doesn’t mention anything about the question or is completely contrary to the correct answer.\n      - For example, when asked “How to terminate a databricks cluster”, the answer is empty string, or content that’s completely irrelevant, or sorry I don’t know the answer.\n  - Score 1: the answer provides some relevance to the question and answers one aspect of the question correctly.\n      - Example:\n          - Question: How to terminate a databricks cluster\n          - Answer: Databricks cluster is a cloud-based computing environment that allows users to process big data and run distributed data processing tasks efficiently.\n          - Or answer:  In the Databricks workspace, navigate to the \"Clusters\" tab. And then this is a hard question that I need to think more about it\n  - Score 2: the answer mostly answer the question but is missing or hallucinating on one critical aspect.\n      - Example:\n          - Question: How to terminate a databricks cluster”\n          - Answer: “In the Databricks workspace, navigate to the \"Clusters\" tab.\n          Find the cluster you want to terminate from the list of active clusters.\n          And then you’ll find a button to terminate all clusters at once”\n  - Score 3: the answer correctly answer the question and not missing any major aspect\n      - Example:\n          - Question: How to terminate a databricks cluster\n          - Answer: In the Databricks workspace, navigate to the \"Clusters\" tab.\n          Find the cluster you want to terminate from the list of active clusters.\n          Click on the down-arrow next to the cluster name to open the cluster details.\n          Click on the \"Terminate\" button. A confirmation dialog will appear. Click \"Terminate\" again to confirm the action.”\n\n2. Comprehensiveness: How comprehensive is the answer, does it fully answer all aspects of the question and provide comprehensive explanation and other necessary information. Below are the details for different scores:\n  - Score 0: typically if the answer is completely incorrect, then the comprehensiveness is also zero score.\n  - Score 1: if the answer is correct but too short to fully answer the question, then we can give score 1 for comprehensiveness.\n      - Example:\n          - Question: How to use databricks API to create a cluster?\n          - Answer: First, you will need a Databricks access token with the appropriate permissions. You can generate this token through the Databricks UI under the 'User Settings' option. And then (the rest is missing)\n  - Score 2: the answer is correct and roughly answer the main aspects of the question, but it’s missing description about details. Or is completely missing details about one minor aspect.\n      - Example:\n          - Question: How to use databricks API to create a cluster?\n          - Answer: You will need a Databricks access token with the appropriate permissions. Then you’ll need to set up the request URL, then you can make the HTTP Request. Then you can handle the request response.\n      - Example:\n          - Question: How to use databricks API to create a cluster?\n          - Answer: You will need a Databricks access token with the appropriate permissions. Then you’ll need to set up the request URL, then you can make the HTTP Request. Then you can handle the request response.\n  - Score 3: the answer is correct, and covers all the main aspects of the question\n\n4. Readability: How readable is the answer, does it have redundant information or incomplete information that hurts the readability of the answer.\n  - Score 0: the answer is completely unreadable, e.g. fully of symbols that’s hard to read; e.g. keeps repeating the words that it’s very hard to understand the meaning of the paragraph. No meaningful information can be extracted from the answer.\n  - Score 1: the answer is slightly readable, there are irrelevant symbols or repeated words, but it can roughly form a meaningful sentence that cover some aspects of the answer.\n      - Example:\n          - Question: How to use databricks API to create a cluster?\n          - Answer: You you  you  you  you  you  will need a Databricks access token with the appropriate permissions. And then then you’ll need to set up the request URL, then you can make the HTTP Request. Then Then Then Then Then Then Then Then Then\n  - Score 2: the answer is correct and mostly readable, but there is one obvious piece that’s affecting the readability (mentioning of irrelevant pieces, repeated words)\n      - Example:\n          - Question: How to terminate a databricks cluster\n          - Answer: In the Databricks workspace, navigate to the \"Clusters\" tab.\n          Find the cluster you want to terminate from the list of active clusters.\n          Click on the down-arrow next to the cluster name to open the cluster details.\n          Click on the \"Terminate\" button…………………………………..\n          A confirmation dialog will appear. Click \"Terminate\" again to confirm the action.\n  - Score 3: the answer is correct and reader friendly, no obvious piece that affect readability.\n\n4. Now provide the final rating using below ratio\n    - Ratio: 60% correctness + 20% comprehensiveness + 20% readability\n\n## User question\n{question}\n\n## Response to be judged\n{response}\n\n## Instruction\nRespond in a below format and do not add any more information, in place of score provide only either a integer of a float with one decimal\n1. <Metric_Name>:\n  - Score: <SCORE>\n  - Explanation: <EXPLANATION>\n\nThen provide the final score\n4. Final rating:\n  - Score: <SCORE>\n",
                                "display_name": "Template",
                                "advanced": false,
                                "dynamic": false,
                                "info": "",
                                "title_case": false,
                                "type": "prompt",
                                "_input_type": "PromptInput"
                            },
                            "question": {
                                "field_type": "str",
                                "required": false,
                                "placeholder": "",
                                "list": false,
                                "show": true,
                                "multiline": true,
                                "value": "",
                                "fileTypes": [],
                                "file_path": "",
                                "name": "question",
                                "display_name": "question",
                                "advanced": false,
                                "input_types": [
                                    "Message",
                                    "Text"
                                ],
                                "dynamic": false,
                                "info": "",
                                "load_from_db": false,
                                "title_case": false,
                                "type": "str"
                            },
                            "response": {
                                "field_type": "str",
                                "required": false,
                                "placeholder": "",
                                "list": false,
                                "show": true,
                                "multiline": true,
                                "value": "",
                                "fileTypes": [],
                                "file_path": "",
                                "name": "response",
                                "display_name": "response",
                                "advanced": false,
                                "input_types": [
                                    "Message",
                                    "Text"
                                ],
                                "dynamic": false,
                                "info": "",
                                "load_from_db": false,
                                "title_case": false,
                                "type": "str"
                            }
                        },
                        "description": "Create a prompt template with dynamic variables.",
                        "icon": "prompts",
                        "is_input": null,
                        "is_output": null,
                        "is_composition": null,
                        "base_classes": [
                            "Message"
                        ],
                        "name": "",
                        "display_name": "Prompt",
                        "documentation": "",
                        "custom_fields": {
                            "template": [
                                "question",
                                "response"
                            ]
                        },
                        "output_types": [],
                        "full_path": null,
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                            {
                                "types": [
                                    "Message"
                                ],
                                "selected": "Message",
                                "name": "prompt",
                                "hidden": null,
                                "display_name": "Prompt Message",
                                "method": "build_prompt",
                                "value": "__UNDEFINED__",
                                "cache": true,
                                "required_inputs": null
                            }
                        ],
                        "field_order": [
                            "template"
                        ],
                        "beta": false,
                        "error": null,
                        "edited": false,
                        "metadata": {},
                        "lf_version": "1.0.19.post2"
                    },
                    "id": "Prompt-k5TEn"
                },
                "selected": false,
                "width": 384,
                "height": 477,
                "positionAbsolute": {
                    "x": 4910.103348280599,
                    "y": 123.2280910726034
                },
                "dragging": false
            },
            {
                "id": "Prompt-b8sx9",
                "type": "genericNode",
                "position": {
                    "x": 947.109618338844,
                    "y": 2323.3068802123325
                },
                "data": {
                    "description": "Create a prompt template with dynamic variables.",
                    "display_name": "Prompt",
                    "id": "Prompt-b8sx9",
                    "node": {
                        "template": {
                            "_type": "Component",
                            "code": {
                                "advanced": true,
                                "dynamic": true,
                                "fileTypes": [],
                                "file_path": "",
                                "info": "",
                                "list": false,
                                "load_from_db": false,
                                "multiline": true,
                                "name": "code",
                                "password": false,
                                "placeholder": "",
                                "required": true,
                                "show": true,
                                "title_case": false,
                                "type": "code",
                                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
                            },
                            "question": {
                                "field_type": "str",
                                "required": false,
                                "placeholder": "",
                                "list": false,
                                "show": true,
                                "multiline": true,
                                "value": "",
                                "fileTypes": [],
                                "file_path": "",
                                "name": "question",
                                "display_name": "question",
                                "advanced": false,
                                "input_types": [
                                    "Message",
                                    "Text"
                                ],
                                "dynamic": false,
                                "info": "",
                                "load_from_db": false,
                                "title_case": false,
                                "type": "str"
                            },
                            "template": {
                                "advanced": false,
                                "display_name": "Template",
                                "dynamic": false,
                                "info": "",
                                "list": false,
                                "load_from_db": false,
                                "name": "template",
                                "placeholder": "",
                                "required": false,
                                "show": true,
                                "title_case": false,
                                "trace_as_input": true,
                                "type": "prompt",
                                "value": "You are a seasoned research assistant tasked with generating search queries to find relevant information for the following task: {question}.\n\nContext:\nWe are writing product management articles for startups\n\nUse this context to inform and refine your search queries. The context provides real-time web information that can help you generate more specific and relevant queries. Consider any current events, recent developments, or specific details mentioned in the context that could enhance the search queries.\n\nAssume the current date is November 25th 2024, if required\n\nExamples:\n- How many startups fails in the first year?\n- How start using shape-up in big companies?\n- How many startups are in Europe?\n- How long a SaaS project takes to develop?\n- When you need to scale your management?\n"
                            }
                        },
                        "description": "Create a prompt template with dynamic variables.",
                        "icon": "prompts",
                        "is_input": null,
                        "is_output": null,
                        "is_composition": null,
                        "base_classes": [
                            "Message"
                        ],
                        "name": "",
                        "display_name": "Prompt",
                        "documentation": "",
                        "custom_fields": {
                            "template": [
                                "question"
                            ]
                        },
                        "output_types": [],
                        "full_path": null,
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                            {
                                "types": [
                                    "Message"
                                ],
                                "selected": "Message",
                                "name": "prompt",
                                "hidden": null,
                                "display_name": "Prompt Message",
                                "method": "build_prompt",
                                "value": "__UNDEFINED__",
                                "cache": true,
                                "required_inputs": null
                            }
                        ],
                        "field_order": [
                            "template"
                        ],
                        "beta": false,
                        "error": null,
                        "edited": false,
                        "metadata": {},
                        "lf_version": "1.0.19.post2"
                    },
                    "type": "Prompt"
                },
                "selected": false,
                "width": 384,
                "height": 391,
                "positionAbsolute": {
                    "x": 947.109618338844,
                    "y": 2323.3068802123325
                },
                "dragging": false
            },
            {
                "id": "MultiQueryRetriever-jZ5YG",
                "type": "genericNode",
                "position": {
                    "x": 1724.3625009787297,
                    "y": 1453.682009869302
                },
                "data": {
                    "type": "MultiQueryRetriever",
                    "node": {
                        "template": {
                            "_type": "Component",
                            "llm": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "llm",
                                "value": "",
                                "display_name": "LLM",
                                "advanced": false,
                                "input_types": [
                                    "LanguageModel"
                                ],
                                "dynamic": false,
                                "info": "LLM to be passed as input.",
                                "title_case": false,
                                "type": "other",
                                "_input_type": "HandleInput"
                            },
                            "retriever": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "retriever",
                                "value": "",
                                "display_name": "Retriever",
                                "advanced": false,
                                "input_types": [
                                    "Retriever"
                                ],
                                "dynamic": false,
                                "info": "",
                                "title_case": false,
                                "type": "other",
                                "_input_type": "HandleInput"
                            },
                            "code": {
                                "type": "code",
                                "required": true,
                                "placeholder": "",
                                "list": false,
                                "show": true,
                                "multiline": true,
                                "value": "from typing import Optional, List\n\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\n\nfrom langflow.custom import Component\n\nfrom langflow.io import Output, HandleInput, MessageTextInput, MultilineInput\nfrom langflow.schema import Data\nfrom langflow.helpers.data import docs_to_data\n\n\nfrom langchain_core.output_parsers import BaseOutputParser\nfrom pydantic import BaseModel, Field\nfrom langchain_core.prompts import PromptTemplate\n\n\n\nclass MultiQueryRetrieverComponent(Component):\n    display_name = \"MultiQueryRetriever\"\n    description = \"Initialize from llm using default template.\"\n    documentation = \"https://python.langchain.com/docs/how_to/MultiQueryRetriever/\"\n    name = \"MultiQueryRetriever\"\n    \n    inputs = [\n        MultilineInput(\n            name=\"search_query\",\n            display_name=\"Query\",\n            info=\"Query to be passed as input.\",\n            input_types=[\"Message\", \"Text\"],\n        ),\n        HandleInput(\n            name=\"llm\",\n            display_name=\"LLM\",\n            info=\"LLM to be passed as input.\",\n            input_types=[\"LanguageModel\"],\n        ),\n        MessageTextInput(name=\"prompt\", display_name=\"Prompt\", value=\"Connect your prompt\"),\n\n        HandleInput(name=\"retriever\", display_name=\"Retriever\", input_types=[\"Retriever\"]),\n    ]\n    \n    outputs = [\n        Output(display_name=\"Retrieved Documents\", name=\"documents\", method=\"retrieve_documents\"),\n    ]\n    \n\n    def retrieve_documents(self) -> List[Data]:\n        QUERY_PROMPT = PromptTemplate(\n            input_variables=[\"question\"],\n            template=self.prompt,\n        )\n        retriever = MultiQueryRetriever.from_llm(\n            retriever=self.retriever, llm=self.llm, prompt=QUERY_PROMPT, include_original=True, parser_key=\"lines\"\n        )  # \"lines\" is the key (attribute name) of the parsed output\n        \n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\n            unique_docs = retriever.invoke(self.search_query)\n            data = docs_to_data(unique_docs)\n            self.status = data\n            \n            return data\n            \n        else:\n            return []\n    \n\n",
                                "fileTypes": [],
                                "file_path": "",
                                "password": false,
                                "name": "code",
                                "advanced": true,
                                "dynamic": true,
                                "info": "",
                                "load_from_db": false,
                                "title_case": false
                            },
                            "prompt": {
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "prompt",
                                "value": "",
                                "display_name": "Prompt",
                                "advanced": false,
                                "input_types": [
                                    "Message"
                                ],
                                "dynamic": false,
                                "info": "",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "MessageTextInput"
                            },
                            "search_query": {
                                "trace_as_input": true,
                                "multiline": true,
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "search_query",
                                "value": "",
                                "display_name": "Query",
                                "advanced": false,
                                "input_types": [
                                    "Message",
                                    "Text"
                                ],
                                "dynamic": false,
                                "info": "Query to be passed as input.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "MultilineInput"
                            }
                        },
                        "description": "Initialize from llm using default template.",
                        "base_classes": [
                            "Data"
                        ],
                        "display_name": "MultiQueryRetriever",
                        "documentation": "https://python.langchain.com/docs/how_to/MultiQueryRetriever/",
                        "custom_fields": {},
                        "output_types": [],
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                            {
                                "types": [
                                    "Data"
                                ],
                                "selected": "Data",
                                "name": "documents",
                                "display_name": "Retrieved Documents",
                                "method": "retrieve_documents",
                                "value": "__UNDEFINED__",
                                "cache": true
                            }
                        ],
                        "field_order": [
                            "search_query",
                            "llm",
                            "prompt",
                            "retriever"
                        ],
                        "beta": false,
                        "edited": true,
                        "metadata": {},
                        "lf_version": "1.0.19.post2"
                    },
                    "id": "MultiQueryRetriever-jZ5YG"
                },
                "selected": false,
                "width": 384,
                "height": 469,
                "positionAbsolute": {
                    "x": 1724.3625009787297,
                    "y": 1453.682009869302
                },
                "dragging": false
            },
            {
                "id": "AmazonKendra-hgFr5",
                "type": "genericNode",
                "position": {
                    "x": 1545.637894044034,
                    "y": 2407.5922071461414
                },
                "data": {
                    "type": "Tavily",
                    "node": {
                        "template": {
                            "_type": "Component",
                            "api_key": {
                                "load_from_db": false,
                                "required": true,
                                "placeholder": "",
                                "show": true,
                                "name": "api_key",
                                "value": "",
                                "display_name": "Tavily API Key",
                                "advanced": false,
                                "input_types": [
                                    "Message"
                                ],
                                "dynamic": false,
                                "info": "Your Tavily API Key.",
                                "title_case": false,
                                "password": true,
                                "type": "str",
                                "_input_type": "SecretStrInput"
                            },
                            "code": {
                                "type": "code",
                                "required": true,
                                "placeholder": "",
                                "list": false,
                                "show": true,
                                "multiline": true,
                                "value": "from typing import cast\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langchain_community.retrievers import TavilySearchAPIRetriever\nfrom langflow.helpers.data import docs_to_data\n\nfrom langflow.io import SecretStrInput, Output, MultilineInput, IntInput\nfrom langflow.custom import CustomComponent\nfrom langflow.field_typing import Retriever\n\n\nclass TavilyRetrieverComponent(LCVectorStoreComponent):\n    display_name: str = \"Tavily Retriever\"\n    description: str = \"Retriever that uses the Tavily Search\"\n    name = \"Tavily\"\n    icon = \"TavilyIcon\"\n\n    inputs = [\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Tavily API Key\",\n            required=True,\n            info=\"Your Tavily API Key.\",\n        ),\n        MultilineInput(\n            name=\"search_query\",\n            display_name=\"Query\",\n            info=\"Query to be passed as input.\",\n            input_types=[\"Message\", \"Text\"],\n        ),\n        IntInput(name=\"top_n\", display_name=\"Top N\", value=5),\n\n        ]\n    \n    outputs = [\n        Output(name=\"base_retriever\", display_name=\"Retriever\", method=\"build_base_retriever\"),\n        Output(display_name=\"Search Results\", name=\"search_results\", method=\"search_documents\")\n        ]\n\n    def build_base_retriever(self) -> Retriever:  # type: ignore[type-var]\n        try:\n            output = TavilySearchAPIRetriever(api_key=self.api_key, k=self.top_n)\n\n        except Exception as e:\n            msg = \"Could not connect to AmazonKendra API.\"\n            raise ValueError(msg) from e\n        return cast(Retriever, output)\n\n    async def search_documents(self) -> list[Data]:\n        retriever = self.build_base_retriever()\n        docs = await retriever.ainvoke(self.search_query)\n        data = docs_to_data(docs)\n        self.status = data\n        return data\n        \n        \n    @check_cached_vector_store\n    def build_vector_store(self) -> VectorStore:\n        msg = \"Cohere Rerank does not support vector stores.\"\n        raise NotImplementedError(msg)",
                                "fileTypes": [],
                                "file_path": "",
                                "password": false,
                                "name": "code",
                                "advanced": true,
                                "dynamic": true,
                                "info": "",
                                "load_from_db": false,
                                "title_case": false
                            },
                            "search_query": {
                                "trace_as_input": true,
                                "multiline": true,
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "search_query",
                                "value": "",
                                "display_name": "Query",
                                "advanced": false,
                                "input_types": [
                                    "Message",
                                    "Text"
                                ],
                                "dynamic": false,
                                "info": "Query to be passed as input.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "MultilineInput"
                            },
                            "top_n": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "top_n",
                                "value": 5,
                                "display_name": "Top N",
                                "advanced": false,
                                "dynamic": false,
                                "info": "",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            }
                        },
                        "description": "Retriever that uses the Tavily Search",
                        "icon": "TavilyIcon",
                        "base_classes": [
                            "Data",
                            "Retriever"
                        ],
                        "display_name": "Tavily Retriever",
                        "documentation": "",
                        "custom_fields": {},
                        "output_types": [],
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                            {
                                "types": [
                                    "Retriever"
                                ],
                                "selected": "Retriever",
                                "name": "base_retriever",
                                "display_name": "Retriever",
                                "method": "build_base_retriever",
                                "value": "__UNDEFINED__",
                                "cache": true
                            },
                            {
                                "types": [
                                    "Data"
                                ],
                                "selected": "Data",
                                "name": "search_results",
                                "display_name": "Search Results",
                                "method": "search_documents",
                                "value": "__UNDEFINED__",
                                "cache": true
                            }
                        ],
                        "field_order": [
                            "api_key",
                            "search_query",
                            "top_n"
                        ],
                        "beta": false,
                        "edited": true,
                        "metadata": {},
                        "lf_version": "1.0.19.post2"
                    },
                    "id": "AmazonKendra-hgFr5"
                },
                "selected": true,
                "width": 384,
                "height": 501,
                "dragging": false,
                "positionAbsolute": {
                    "x": 1545.637894044034,
                    "y": 2407.5922071461414
                }
            },
            {
                "id": "ParseData-Sdfos",
                "type": "genericNode",
                "position": {
                    "x": 2191.0980460273304,
                    "y": 1338.9526812169433
                },
                "data": {
                    "description": "Convert Data into plain text following a specified template.",
                    "display_name": "Parse Data",
                    "id": "ParseData-Sdfos",
                    "node": {
                        "base_classes": [
                            "Message"
                        ],
                        "beta": false,
                        "conditional_paths": [],
                        "custom_fields": {},
                        "description": "Convert Data into plain text following a specified template.",
                        "display_name": "Parse Data",
                        "documentation": "",
                        "edited": false,
                        "field_order": [
                            "data",
                            "template",
                            "sep"
                        ],
                        "frozen": false,
                        "icon": "braces",
                        "metadata": {},
                        "output_types": [],
                        "outputs": [
                            {
                                "cache": true,
                                "display_name": "Text",
                                "method": "parse_data",
                                "name": "text",
                                "selected": "Message",
                                "types": [
                                    "Message"
                                ],
                                "value": "__UNDEFINED__"
                            }
                        ],
                        "pinned": false,
                        "template": {
                            "_type": "Component",
                            "code": {
                                "advanced": true,
                                "dynamic": true,
                                "fileTypes": [],
                                "file_path": "",
                                "info": "",
                                "list": false,
                                "load_from_db": false,
                                "multiline": true,
                                "name": "code",
                                "password": false,
                                "placeholder": "",
                                "required": true,
                                "show": true,
                                "title_case": false,
                                "type": "code",
                                "value": "from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. \"\n            \"It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n"
                            },
                            "data": {
                                "advanced": false,
                                "display_name": "Data",
                                "dynamic": false,
                                "info": "The data to convert to text.",
                                "input_types": [
                                    "Data"
                                ],
                                "list": false,
                                "name": "data",
                                "placeholder": "",
                                "required": false,
                                "show": true,
                                "title_case": false,
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "type": "other",
                                "value": ""
                            },
                            "sep": {
                                "advanced": true,
                                "display_name": "Separator",
                                "dynamic": false,
                                "info": "",
                                "list": false,
                                "load_from_db": false,
                                "name": "sep",
                                "placeholder": "",
                                "required": false,
                                "show": true,
                                "title_case": false,
                                "trace_as_metadata": true,
                                "type": "str",
                                "value": "\n"
                            },
                            "template": {
                                "advanced": false,
                                "display_name": "Template",
                                "dynamic": false,
                                "info": "The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.",
                                "input_types": [
                                    "Message"
                                ],
                                "list": false,
                                "load_from_db": false,
                                "multiline": true,
                                "name": "template",
                                "placeholder": "",
                                "required": false,
                                "show": true,
                                "title_case": false,
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "type": "str",
                                "value": "<website title={title}>\n{text}\n</website>"
                            }
                        },
                        "lf_version": "1.0.19.post2"
                    },
                    "type": "ParseData"
                },
                "selected": false,
                "width": 384,
                "height": 351,
                "positionAbsolute": {
                    "x": 2191.0980460273304,
                    "y": 1338.9526812169433
                },
                "dragging": false
            },
            {
                "id": "Prompt-JKqdq",
                "type": "genericNode",
                "position": {
                    "x": 2709.1830307614487,
                    "y": 706.0171337685994
                },
                "data": {
                    "description": "Create a prompt template with dynamic variables.",
                    "display_name": "Prompt",
                    "id": "Prompt-JKqdq",
                    "node": {
                        "template": {
                            "_type": "Component",
                            "code": {
                                "advanced": true,
                                "dynamic": true,
                                "fileTypes": [],
                                "file_path": "",
                                "info": "",
                                "list": false,
                                "load_from_db": false,
                                "multiline": true,
                                "name": "code",
                                "password": false,
                                "placeholder": "",
                                "required": true,
                                "show": true,
                                "title_case": false,
                                "type": "code",
                                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
                            },
                            "context": {
                                "field_type": "str",
                                "required": false,
                                "placeholder": "",
                                "list": false,
                                "show": true,
                                "multiline": true,
                                "value": "",
                                "fileTypes": [],
                                "file_path": "",
                                "name": "context",
                                "display_name": "context",
                                "advanced": false,
                                "input_types": [
                                    "Message",
                                    "Text"
                                ],
                                "dynamic": false,
                                "info": "",
                                "load_from_db": false,
                                "title_case": false,
                                "type": "str"
                            },
                            "question": {
                                "field_type": "str",
                                "required": false,
                                "placeholder": "",
                                "list": false,
                                "show": true,
                                "multiline": true,
                                "value": "",
                                "fileTypes": [],
                                "file_path": "",
                                "name": "question",
                                "display_name": "question",
                                "advanced": false,
                                "input_types": [
                                    "Message",
                                    "Text"
                                ],
                                "dynamic": false,
                                "info": "",
                                "load_from_db": false,
                                "title_case": false,
                                "type": "str"
                            },
                            "template": {
                                "advanced": false,
                                "display_name": "Template",
                                "dynamic": false,
                                "info": "",
                                "list": false,
                                "load_from_db": false,
                                "name": "template",
                                "placeholder": "",
                                "required": false,
                                "show": true,
                                "title_case": false,
                                "trace_as_input": true,
                                "type": "prompt",
                                "value": "## General context\n{context}\n\n## Internet insights\n{insights}\n\n## Instruction\nUsing the above information, generate an outline for a research report in Markdown syntax for the following question or topic: \n```\n{question}\n```\n\nThe outline should provide a well-structured framework for the research product management articles, including the main sections, subsections, and key points to be covered. The article should be detailed, informative, in-depth, and a minimum of 2000 words\n\nUse appropriate Markdown syntax to format the outline and ensure readability. Do not show the table of content in the beginning, just the detailed outline"
                            },
                            "insights": {
                                "field_type": "str",
                                "required": false,
                                "placeholder": "",
                                "list": false,
                                "show": true,
                                "multiline": true,
                                "value": "",
                                "fileTypes": [],
                                "file_path": "",
                                "name": "insights",
                                "display_name": "insights",
                                "advanced": false,
                                "input_types": [
                                    "Message",
                                    "Text"
                                ],
                                "dynamic": false,
                                "info": "",
                                "load_from_db": false,
                                "title_case": false,
                                "type": "str"
                            }
                        },
                        "description": "Create a prompt template with dynamic variables.",
                        "icon": "prompts",
                        "is_input": null,
                        "is_output": null,
                        "is_composition": null,
                        "base_classes": [
                            "Message"
                        ],
                        "name": "",
                        "display_name": "Prompt",
                        "documentation": "",
                        "custom_fields": {
                            "template": [
                                "context",
                                "insights",
                                "question"
                            ]
                        },
                        "output_types": [],
                        "full_path": null,
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                            {
                                "types": [
                                    "Message"
                                ],
                                "selected": "Message",
                                "name": "prompt",
                                "hidden": null,
                                "display_name": "Prompt Message",
                                "method": "build_prompt",
                                "value": "__UNDEFINED__",
                                "cache": true,
                                "required_inputs": null
                            }
                        ],
                        "field_order": [
                            "template"
                        ],
                        "beta": false,
                        "error": null,
                        "edited": false,
                        "metadata": {},
                        "lf_version": "1.0.19.post2"
                    },
                    "type": "Prompt"
                },
                "selected": false,
                "width": 384,
                "height": 563,
                "positionAbsolute": {
                    "x": 2709.1830307614487,
                    "y": 706.0171337685994
                },
                "dragging": false
            },
            {
                "id": "QdrantVectorStoreComponent-e7yEx",
                "type": "genericNode",
                "position": {
                    "x": 926.9286878479103,
                    "y": 86.83109657687942
                },
                "data": {
                    "type": "QdrantVectorStoreComponent",
                    "node": {
                        "template": {
                            "_type": "Component",
                            "embedding": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "embedding",
                                "value": "",
                                "display_name": "Embedding",
                                "advanced": false,
                                "input_types": [
                                    "Embeddings"
                                ],
                                "dynamic": false,
                                "info": "",
                                "title_case": false,
                                "type": "other",
                                "_input_type": "HandleInput"
                            },
                            "ingest_data": {
                                "trace_as_metadata": true,
                                "list": true,
                                "trace_as_input": true,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "ingest_data",
                                "value": "",
                                "display_name": "Ingest Data",
                                "advanced": false,
                                "input_types": [
                                    "Data"
                                ],
                                "dynamic": false,
                                "info": "",
                                "title_case": false,
                                "type": "other",
                                "_input_type": "DataInput"
                            },
                            "code": {
                                "type": "code",
                                "required": true,
                                "placeholder": "",
                                "list": false,
                                "show": true,
                                "multiline": true,
                                "value": "from langchain.embeddings.base import Embeddings\nfrom langchain_community.vectorstores import Qdrant\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import (\n    DataInput,\n    DropdownInput,\n    HandleInput,\n    IntInput,\n    MultilineInput,\n    SecretStrInput,\n    StrInput,\n)\nfrom langflow.schema import Data\n\n\nclass QdrantVectorStoreComponent(LCVectorStoreComponent):\n    display_name = \"Qdrant\"\n    description = \"Qdrant Vector Store with search capabilities\"\n    documentation = \"https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/qdrant\"\n    icon = \"Qdrant\"\n\n    inputs = [\n        StrInput(name=\"collection_name\", display_name=\"Collection Name\", required=True),\n        StrInput(name=\"url\", display_name=\"URL\"),\n        DropdownInput(\n            name=\"distance_func\",\n            display_name=\"Distance Function\",\n            options=[\"Cosine\", \"Euclidean\", \"Dot Product\"],\n            value=\"Cosine\",\n            advanced=True,\n        ),\n        StrInput(name=\"content_payload_key\", display_name=\"Content Payload Key\", value=\"page_content\", advanced=True),\n        StrInput(name=\"metadata_payload_key\", display_name=\"Metadata Payload Key\", value=\"metadata\", advanced=True),\n        MultilineInput(name=\"search_query\", display_name=\"Search Query\"),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingest Data\",\n            is_list=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            value=4,\n            advanced=True,\n        ),\n    ]\n\n    @check_cached_vector_store\n    def build_vector_store(self) -> Qdrant:\n        qdrant_kwargs = {\n            \"collection_name\": self.collection_name,\n            \"content_payload_key\": self.content_payload_key,\n            \"metadata_payload_key\": self.metadata_payload_key,\n        }\n\n        server_kwargs = {\n            \"url\": self.url or None,\n        }\n\n        server_kwargs = {k: v for k, v in server_kwargs.items() if v is not None}\n        documents = []\n\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n            else:\n                documents.append(_input)\n\n        if not isinstance(self.embedding, Embeddings):\n            msg = \"Invalid embedding object\"\n            raise TypeError(msg)\n\n        if documents:\n            qdrant = Qdrant.from_documents(documents, embedding=self.embedding, **qdrant_kwargs, **server_kwargs)\n        else:\n            from qdrant_client import QdrantClient\n\n            client = QdrantClient(**server_kwargs)\n            qdrant = Qdrant(embeddings=self.embedding, client=client, **qdrant_kwargs)\n\n        return qdrant\n\n    def search_documents(self) -> list[Data]:\n        vector_store = self.build_vector_store()\n\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\n            docs = vector_store.similarity_search(\n                query=self.search_query,\n                k=self.number_of_results,\n            )\n\n            data = docs_to_data(docs)\n            self.status = data\n            return data\n        return []\n",
                                "fileTypes": [],
                                "file_path": "",
                                "password": false,
                                "name": "code",
                                "advanced": true,
                                "dynamic": true,
                                "info": "",
                                "load_from_db": false,
                                "title_case": false
                            },
                            "collection_name": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": true,
                                "placeholder": "",
                                "show": true,
                                "name": "collection_name",
                                "value": "product-management",
                                "display_name": "Collection Name",
                                "advanced": false,
                                "dynamic": false,
                                "info": "",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "content_payload_key": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "content_payload_key",
                                "value": "page_content",
                                "display_name": "Content Payload Key",
                                "advanced": true,
                                "dynamic": false,
                                "info": "",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "distance_func": {
                                "trace_as_metadata": true,
                                "options": [
                                    "Cosine",
                                    "Euclidean",
                                    "Dot Product"
                                ],
                                "combobox": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "distance_func",
                                "value": "Cosine",
                                "display_name": "Distance Function",
                                "advanced": true,
                                "dynamic": false,
                                "info": "",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "DropdownInput"
                            },
                            "metadata_payload_key": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "metadata_payload_key",
                                "value": "metadata",
                                "display_name": "Metadata Payload Key",
                                "advanced": true,
                                "dynamic": false,
                                "info": "",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "number_of_results": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "number_of_results",
                                "value": 5,
                                "display_name": "Number of Results",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Number of results to return.",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput",
                                "load_from_db": false
                            },
                            "search_query": {
                                "trace_as_input": true,
                                "multiline": true,
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "search_query",
                                "value": "",
                                "display_name": "Search Query",
                                "advanced": false,
                                "input_types": [
                                    "Message"
                                ],
                                "dynamic": false,
                                "info": "",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "MultilineInput"
                            },
                            "url": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "url",
                                "value": "http://qdrant:6333",
                                "display_name": "URL",
                                "advanced": false,
                                "dynamic": false,
                                "info": "",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            }
                        },
                        "description": "Qdrant Vector Store with search capabilities",
                        "icon": "Qdrant",
                        "base_classes": [
                            "Data",
                            "Retriever",
                            "VectorStore"
                        ],
                        "display_name": "Qdrant",
                        "documentation": "https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/qdrant",
                        "custom_fields": {},
                        "output_types": [],
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                            {
                                "types": [
                                    "Retriever"
                                ],
                                "selected": "Retriever",
                                "name": "base_retriever",
                                "display_name": "Retriever",
                                "method": "build_base_retriever",
                                "value": "__UNDEFINED__",
                                "cache": true,
                                "required_inputs": []
                            },
                            {
                                "types": [
                                    "Data"
                                ],
                                "selected": "Data",
                                "name": "search_results",
                                "display_name": "Search Results",
                                "method": "search_documents",
                                "value": "__UNDEFINED__",
                                "cache": true,
                                "required_inputs": [
                                    "collection_name",
                                    "content_payload_key",
                                    "embedding",
                                    "ingest_data",
                                    "metadata_payload_key",
                                    "number_of_results",
                                    "search_query",
                                    "url"
                                ]
                            },
                            {
                                "types": [
                                    "VectorStore"
                                ],
                                "selected": "VectorStore",
                                "name": "vector_store",
                                "display_name": "Vector Store",
                                "method": "cast_vector_store",
                                "value": "__UNDEFINED__",
                                "cache": true,
                                "required_inputs": []
                            }
                        ],
                        "field_order": [
                            "collection_name",
                            "url",
                            "distance_func",
                            "content_payload_key",
                            "metadata_payload_key",
                            "search_query",
                            "ingest_data",
                            "embedding",
                            "number_of_results"
                        ],
                        "beta": false,
                        "edited": true,
                        "metadata": {},
                        "lf_version": "1.0.19.post2"
                    },
                    "id": "QdrantVectorStoreComponent-e7yEx"
                },
                "selected": false,
                "width": 384,
                "height": 637,
                "positionAbsolute": {
                    "x": 926.9286878479103,
                    "y": 86.83109657687942
                },
                "dragging": false
            },
            {
                "id": "OllamaModel-d0LkQ",
                "type": "genericNode",
                "position": {
                    "x": 968.4879905395496,
                    "y": 1610.9910563091735
                },
                "data": {
                    "type": "OllamaModel",
                    "node": {
                        "template": {
                            "_type": "Component",
                            "output_parser": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "output_parser",
                                "value": "",
                                "display_name": "Output Parser",
                                "advanced": true,
                                "input_types": [
                                    "OutputParser"
                                ],
                                "dynamic": false,
                                "info": "The parser to use to parse the output of the model",
                                "title_case": false,
                                "type": "other",
                                "_input_type": "HandleInput"
                            },
                            "base_url": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "base_url",
                                "value": "http://ollama:11434",
                                "display_name": "Base URL",
                                "advanced": false,
                                "dynamic": false,
                                "info": "Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "code": {
                                "type": "code",
                                "required": true,
                                "placeholder": "",
                                "list": false,
                                "show": true,
                                "multiline": true,
                                "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs.inputs import HandleInput\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value)\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/api/tags\")\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"name\"] for model in data.get(\"models\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure Ollama is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"llama3.1\",\n            info=\"Refer to https://ollama.com/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\", display_name=\"Format\", info=\"Specify the format of the output (e.g., json).\", advanced=True\n        ),\n        DictInput(name=\"metadata\", display_name=\"Metadata\", info=\"Metadata to add to the run trace.\", advanced=True),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(name=\"tfs_z\", display_name=\"TFS Z\", info=\"Tail free sampling value. (Default: 1)\", advanced=True),\n        IntInput(name=\"timeout\", display_name=\"Timeout\", info=\"Timeout for the request stream.\", advanced=True),\n        IntInput(\n            name=\"top_k\", display_name=\"Top K\", info=\"Limits token selection to top K. (Default: 40)\", advanced=True\n        ),\n        FloatInput(name=\"top_p\", display_name=\"Top P\", info=\"Works together with top-k. (Default: 0.9)\", advanced=True),\n        BoolInput(name=\"verbose\", display_name=\"Verbose\", info=\"Whether to print out response text.\"),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(name=\"system\", display_name=\"System\", info=\"System to use for generating text.\", advanced=True),\n        StrInput(name=\"template\", display_name=\"Template\", info=\"Template to use for generating text.\", advanced=True),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)\n        except Exception as e:\n            msg = \"Could not initialize Ollama LLM.\"\n            raise ValueError(msg) from e\n\n        return output\n",
                                "fileTypes": [],
                                "file_path": "",
                                "password": false,
                                "name": "code",
                                "advanced": true,
                                "dynamic": true,
                                "info": "",
                                "load_from_db": false,
                                "title_case": false
                            },
                            "format": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "format",
                                "value": "",
                                "display_name": "Format",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Specify the format of the output (e.g., json).",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "input_value": {
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "input_value",
                                "value": "",
                                "display_name": "Input",
                                "advanced": false,
                                "input_types": [
                                    "Message"
                                ],
                                "dynamic": false,
                                "info": "",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "MessageInput"
                            },
                            "metadata": {
                                "trace_as_input": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "metadata",
                                "value": {},
                                "display_name": "Metadata",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Metadata to add to the run trace.",
                                "title_case": false,
                                "type": "dict",
                                "_input_type": "DictInput"
                            },
                            "mirostat": {
                                "trace_as_metadata": true,
                                "options": [
                                    "Disabled",
                                    "Mirostat",
                                    "Mirostat 2.0"
                                ],
                                "combobox": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "mirostat",
                                "value": "Disabled",
                                "display_name": "Mirostat",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Enable/disable Mirostat sampling for controlling perplexity.",
                                "real_time_refresh": true,
                                "title_case": false,
                                "type": "str",
                                "_input_type": "DropdownInput"
                            },
                            "mirostat_eta": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "mirostat_eta",
                                "value": "",
                                "display_name": "Mirostat Eta",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Learning rate for Mirostat algorithm. (Default: 0.1)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "mirostat_tau": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "mirostat_tau",
                                "value": "",
                                "display_name": "Mirostat Tau",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Controls the balance between coherence and diversity of the output. (Default: 5.0)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "model_name": {
                                "trace_as_metadata": true,
                                "options": [
                                    "gemma2:2b"
                                ],
                                "combobox": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "model_name",
                                "value": "gemma2:2b",
                                "display_name": "Model Name",
                                "advanced": false,
                                "dynamic": false,
                                "info": "Refer to https://ollama.com/library for more models.",
                                "refresh_button": true,
                                "title_case": false,
                                "type": "str",
                                "_input_type": "DropdownInput"
                            },
                            "num_ctx": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "num_ctx",
                                "value": "",
                                "display_name": "Context Window Size",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Size of the context window for generating tokens. (Default: 2048)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "num_gpu": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "num_gpu",
                                "value": "",
                                "display_name": "Number of GPUs",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "num_thread": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "num_thread",
                                "value": "",
                                "display_name": "Number of Threads",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Number of threads to use during computation. (Default: detected for optimal performance)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "repeat_last_n": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "repeat_last_n",
                                "value": "",
                                "display_name": "Repeat Last N",
                                "advanced": true,
                                "dynamic": false,
                                "info": "How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "repeat_penalty": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "repeat_penalty",
                                "value": "",
                                "display_name": "Repeat Penalty",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Penalty for repetitions in generated text. (Default: 1.1)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "stop_tokens": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "stop_tokens",
                                "value": "",
                                "display_name": "Stop Tokens",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Comma-separated list of tokens to signal the model to stop generating text.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "stream": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "stream",
                                "value": false,
                                "display_name": "Stream",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Stream the response from the model. Streaming works only in Chat.",
                                "title_case": false,
                                "type": "bool",
                                "_input_type": "BoolInput"
                            },
                            "system": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "system",
                                "value": "",
                                "display_name": "System",
                                "advanced": true,
                                "dynamic": false,
                                "info": "System to use for generating text.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "system_message": {
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "system_message",
                                "value": "",
                                "display_name": "System Message",
                                "advanced": true,
                                "input_types": [
                                    "Message"
                                ],
                                "dynamic": false,
                                "info": "System message to pass to the model.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "MessageTextInput"
                            },
                            "tags": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "tags",
                                "value": "",
                                "display_name": "Tags",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Comma-separated list of tags to add to the run trace.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "temperature": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "temperature",
                                "value": 0.2,
                                "display_name": "Temperature",
                                "advanced": false,
                                "dynamic": false,
                                "info": "Controls the creativity of model responses.",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "template": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "template",
                                "value": "",
                                "display_name": "Template",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Template to use for generating text.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "tfs_z": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "tfs_z",
                                "value": "",
                                "display_name": "TFS Z",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Tail free sampling value. (Default: 1)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "timeout": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "timeout",
                                "value": "",
                                "display_name": "Timeout",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Timeout for the request stream.",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "top_k": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "top_k",
                                "value": "",
                                "display_name": "Top K",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Limits token selection to top K. (Default: 40)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "top_p": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "top_p",
                                "value": "",
                                "display_name": "Top P",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Works together with top-k. (Default: 0.9)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "verbose": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "verbose",
                                "value": false,
                                "display_name": "Verbose",
                                "advanced": false,
                                "dynamic": false,
                                "info": "Whether to print out response text.",
                                "title_case": false,
                                "type": "bool",
                                "_input_type": "BoolInput"
                            }
                        },
                        "description": "Generate text using Ollama Local LLMs.",
                        "icon": "Ollama",
                        "base_classes": [
                            "LanguageModel",
                            "Message"
                        ],
                        "display_name": "Ollama",
                        "documentation": "",
                        "custom_fields": {},
                        "output_types": [],
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                            {
                                "types": [
                                    "Message"
                                ],
                                "selected": "Message",
                                "name": "text_output",
                                "display_name": "Text",
                                "method": "text_response",
                                "value": "__UNDEFINED__",
                                "cache": true,
                                "required_inputs": [
                                    "input_value",
                                    "stream",
                                    "system_message"
                                ]
                            },
                            {
                                "types": [
                                    "LanguageModel"
                                ],
                                "selected": "LanguageModel",
                                "name": "model_output",
                                "display_name": "Language Model",
                                "method": "build_model",
                                "value": "__UNDEFINED__",
                                "cache": true,
                                "required_inputs": [
                                    "base_url",
                                    "format",
                                    "metadata",
                                    "mirostat",
                                    "mirostat_eta",
                                    "mirostat_tau",
                                    "model_name",
                                    "num_ctx",
                                    "num_gpu",
                                    "num_thread",
                                    "repeat_last_n",
                                    "repeat_penalty",
                                    "stop_tokens",
                                    "system",
                                    "tags",
                                    "temperature",
                                    "template",
                                    "tfs_z",
                                    "timeout",
                                    "top_k",
                                    "top_p",
                                    "verbose"
                                ]
                            }
                        ],
                        "field_order": [
                            "input_value",
                            "system_message",
                            "stream",
                            "base_url",
                            "model_name",
                            "temperature",
                            "format",
                            "metadata",
                            "mirostat",
                            "mirostat_eta",
                            "mirostat_tau",
                            "num_ctx",
                            "num_gpu",
                            "num_thread",
                            "repeat_last_n",
                            "repeat_penalty",
                            "tfs_z",
                            "timeout",
                            "top_k",
                            "top_p",
                            "verbose",
                            "tags",
                            "stop_tokens",
                            "system",
                            "template",
                            "output_parser"
                        ],
                        "beta": false,
                        "edited": false,
                        "metadata": {},
                        "lf_version": "1.0.19.post2"
                    },
                    "id": "OllamaModel-d0LkQ"
                },
                "selected": false,
                "width": 384,
                "height": 667,
                "positionAbsolute": {
                    "x": 968.4879905395496,
                    "y": 1610.9910563091735
                },
                "dragging": false
            },
            {
                "id": "OllamaModel-VF7Fh",
                "type": "genericNode",
                "position": {
                    "x": 963.1035256590601,
                    "y": 777.4381655592188
                },
                "data": {
                    "type": "OllamaModel",
                    "node": {
                        "template": {
                            "_type": "Component",
                            "output_parser": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "output_parser",
                                "value": "",
                                "display_name": "Output Parser",
                                "advanced": true,
                                "input_types": [
                                    "OutputParser"
                                ],
                                "dynamic": false,
                                "info": "The parser to use to parse the output of the model",
                                "title_case": false,
                                "type": "other",
                                "_input_type": "HandleInput"
                            },
                            "base_url": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "base_url",
                                "value": "http://ollama:11434",
                                "display_name": "Base URL",
                                "advanced": false,
                                "dynamic": false,
                                "info": "Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "code": {
                                "type": "code",
                                "required": true,
                                "placeholder": "",
                                "list": false,
                                "show": true,
                                "multiline": true,
                                "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs.inputs import HandleInput\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value)\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/api/tags\")\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"name\"] for model in data.get(\"models\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure Ollama is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"llama3.1\",\n            info=\"Refer to https://ollama.com/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\", display_name=\"Format\", info=\"Specify the format of the output (e.g., json).\", advanced=True\n        ),\n        DictInput(name=\"metadata\", display_name=\"Metadata\", info=\"Metadata to add to the run trace.\", advanced=True),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(name=\"tfs_z\", display_name=\"TFS Z\", info=\"Tail free sampling value. (Default: 1)\", advanced=True),\n        IntInput(name=\"timeout\", display_name=\"Timeout\", info=\"Timeout for the request stream.\", advanced=True),\n        IntInput(\n            name=\"top_k\", display_name=\"Top K\", info=\"Limits token selection to top K. (Default: 40)\", advanced=True\n        ),\n        FloatInput(name=\"top_p\", display_name=\"Top P\", info=\"Works together with top-k. (Default: 0.9)\", advanced=True),\n        BoolInput(name=\"verbose\", display_name=\"Verbose\", info=\"Whether to print out response text.\"),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(name=\"system\", display_name=\"System\", info=\"System to use for generating text.\", advanced=True),\n        StrInput(name=\"template\", display_name=\"Template\", info=\"Template to use for generating text.\", advanced=True),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)\n        except Exception as e:\n            msg = \"Could not initialize Ollama LLM.\"\n            raise ValueError(msg) from e\n\n        return output\n",
                                "fileTypes": [],
                                "file_path": "",
                                "password": false,
                                "name": "code",
                                "advanced": true,
                                "dynamic": true,
                                "info": "",
                                "load_from_db": false,
                                "title_case": false
                            },
                            "format": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "format",
                                "value": "",
                                "display_name": "Format",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Specify the format of the output (e.g., json).",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "input_value": {
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "input_value",
                                "value": "",
                                "display_name": "Input",
                                "advanced": false,
                                "input_types": [
                                    "Message"
                                ],
                                "dynamic": false,
                                "info": "",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "MessageInput"
                            },
                            "metadata": {
                                "trace_as_input": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "metadata",
                                "value": {},
                                "display_name": "Metadata",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Metadata to add to the run trace.",
                                "title_case": false,
                                "type": "dict",
                                "_input_type": "DictInput"
                            },
                            "mirostat": {
                                "trace_as_metadata": true,
                                "options": [
                                    "Disabled",
                                    "Mirostat",
                                    "Mirostat 2.0"
                                ],
                                "combobox": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "mirostat",
                                "value": "Disabled",
                                "display_name": "Mirostat",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Enable/disable Mirostat sampling for controlling perplexity.",
                                "real_time_refresh": true,
                                "title_case": false,
                                "type": "str",
                                "_input_type": "DropdownInput"
                            },
                            "mirostat_eta": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "mirostat_eta",
                                "value": "",
                                "display_name": "Mirostat Eta",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Learning rate for Mirostat algorithm. (Default: 0.1)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "mirostat_tau": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "mirostat_tau",
                                "value": "",
                                "display_name": "Mirostat Tau",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Controls the balance between coherence and diversity of the output. (Default: 5.0)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "model_name": {
                                "trace_as_metadata": true,
                                "options": [
                                    "gemma2:2b"
                                ],
                                "combobox": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "model_name",
                                "value": "gemma2:2b",
                                "display_name": "Model Name",
                                "advanced": false,
                                "dynamic": false,
                                "info": "Refer to https://ollama.com/library for more models.",
                                "refresh_button": true,
                                "title_case": false,
                                "type": "str",
                                "_input_type": "DropdownInput"
                            },
                            "num_ctx": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "num_ctx",
                                "value": "",
                                "display_name": "Context Window Size",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Size of the context window for generating tokens. (Default: 2048)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "num_gpu": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "num_gpu",
                                "value": "",
                                "display_name": "Number of GPUs",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "num_thread": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "num_thread",
                                "value": "",
                                "display_name": "Number of Threads",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Number of threads to use during computation. (Default: detected for optimal performance)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "repeat_last_n": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "repeat_last_n",
                                "value": "",
                                "display_name": "Repeat Last N",
                                "advanced": true,
                                "dynamic": false,
                                "info": "How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "repeat_penalty": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "repeat_penalty",
                                "value": "",
                                "display_name": "Repeat Penalty",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Penalty for repetitions in generated text. (Default: 1.1)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "stop_tokens": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "stop_tokens",
                                "value": "",
                                "display_name": "Stop Tokens",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Comma-separated list of tokens to signal the model to stop generating text.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "stream": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "stream",
                                "value": false,
                                "display_name": "Stream",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Stream the response from the model. Streaming works only in Chat.",
                                "title_case": false,
                                "type": "bool",
                                "_input_type": "BoolInput"
                            },
                            "system": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "system",
                                "value": "",
                                "display_name": "System",
                                "advanced": true,
                                "dynamic": false,
                                "info": "System to use for generating text.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "system_message": {
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "system_message",
                                "value": "",
                                "display_name": "System Message",
                                "advanced": true,
                                "input_types": [
                                    "Message"
                                ],
                                "dynamic": false,
                                "info": "System message to pass to the model.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "MessageTextInput"
                            },
                            "tags": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "tags",
                                "value": "",
                                "display_name": "Tags",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Comma-separated list of tags to add to the run trace.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "temperature": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "temperature",
                                "value": 0.2,
                                "display_name": "Temperature",
                                "advanced": false,
                                "dynamic": false,
                                "info": "Controls the creativity of model responses.",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "template": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "template",
                                "value": "",
                                "display_name": "Template",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Template to use for generating text.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "tfs_z": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "tfs_z",
                                "value": "",
                                "display_name": "TFS Z",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Tail free sampling value. (Default: 1)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "timeout": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "timeout",
                                "value": "",
                                "display_name": "Timeout",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Timeout for the request stream.",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "top_k": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "top_k",
                                "value": "",
                                "display_name": "Top K",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Limits token selection to top K. (Default: 40)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "top_p": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "top_p",
                                "value": "",
                                "display_name": "Top P",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Works together with top-k. (Default: 0.9)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "verbose": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "verbose",
                                "value": false,
                                "display_name": "Verbose",
                                "advanced": false,
                                "dynamic": false,
                                "info": "Whether to print out response text.",
                                "title_case": false,
                                "type": "bool",
                                "_input_type": "BoolInput"
                            }
                        },
                        "description": "Generate text using Ollama Local LLMs.",
                        "icon": "Ollama",
                        "base_classes": [
                            "LanguageModel",
                            "Message"
                        ],
                        "display_name": "Ollama",
                        "documentation": "",
                        "custom_fields": {},
                        "output_types": [],
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                            {
                                "types": [
                                    "Message"
                                ],
                                "selected": "Message",
                                "name": "text_output",
                                "display_name": "Text",
                                "method": "text_response",
                                "value": "__UNDEFINED__",
                                "cache": true,
                                "required_inputs": [
                                    "input_value",
                                    "stream",
                                    "system_message"
                                ]
                            },
                            {
                                "types": [
                                    "LanguageModel"
                                ],
                                "selected": "LanguageModel",
                                "name": "model_output",
                                "display_name": "Language Model",
                                "method": "build_model",
                                "value": "__UNDEFINED__",
                                "cache": true,
                                "required_inputs": [
                                    "base_url",
                                    "format",
                                    "metadata",
                                    "mirostat",
                                    "mirostat_eta",
                                    "mirostat_tau",
                                    "model_name",
                                    "num_ctx",
                                    "num_gpu",
                                    "num_thread",
                                    "repeat_last_n",
                                    "repeat_penalty",
                                    "stop_tokens",
                                    "system",
                                    "tags",
                                    "temperature",
                                    "template",
                                    "tfs_z",
                                    "timeout",
                                    "top_k",
                                    "top_p",
                                    "verbose"
                                ]
                            }
                        ],
                        "field_order": [
                            "input_value",
                            "system_message",
                            "stream",
                            "base_url",
                            "model_name",
                            "temperature",
                            "format",
                            "metadata",
                            "mirostat",
                            "mirostat_eta",
                            "mirostat_tau",
                            "num_ctx",
                            "num_gpu",
                            "num_thread",
                            "repeat_last_n",
                            "repeat_penalty",
                            "tfs_z",
                            "timeout",
                            "top_k",
                            "top_p",
                            "verbose",
                            "tags",
                            "stop_tokens",
                            "system",
                            "template",
                            "output_parser"
                        ],
                        "beta": false,
                        "edited": false,
                        "metadata": {},
                        "lf_version": "1.0.19.post2"
                    },
                    "id": "OllamaModel-VF7Fh"
                },
                "selected": false,
                "width": 384,
                "height": 667,
                "positionAbsolute": {
                    "x": 963.1035256590601,
                    "y": 777.4381655592188
                },
                "dragging": false
            },
            {
                "id": "OllamaModel-8op10",
                "type": "genericNode",
                "position": {
                    "x": 3248.8314917966086,
                    "y": 420.56799298236155
                },
                "data": {
                    "type": "OllamaModel",
                    "node": {
                        "template": {
                            "_type": "Component",
                            "output_parser": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "output_parser",
                                "value": "",
                                "display_name": "Output Parser",
                                "advanced": true,
                                "input_types": [
                                    "OutputParser"
                                ],
                                "dynamic": false,
                                "info": "The parser to use to parse the output of the model",
                                "title_case": false,
                                "type": "other",
                                "_input_type": "HandleInput"
                            },
                            "base_url": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "base_url",
                                "value": "http://ollama:11434",
                                "display_name": "Base URL",
                                "advanced": false,
                                "dynamic": false,
                                "info": "Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "code": {
                                "type": "code",
                                "required": true,
                                "placeholder": "",
                                "list": false,
                                "show": true,
                                "multiline": true,
                                "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs.inputs import HandleInput\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value)\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/api/tags\")\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"name\"] for model in data.get(\"models\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure Ollama is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"llama3.1\",\n            info=\"Refer to https://ollama.com/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\", display_name=\"Format\", info=\"Specify the format of the output (e.g., json).\", advanced=True\n        ),\n        DictInput(name=\"metadata\", display_name=\"Metadata\", info=\"Metadata to add to the run trace.\", advanced=True),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(name=\"tfs_z\", display_name=\"TFS Z\", info=\"Tail free sampling value. (Default: 1)\", advanced=True),\n        IntInput(name=\"timeout\", display_name=\"Timeout\", info=\"Timeout for the request stream.\", advanced=True),\n        IntInput(\n            name=\"top_k\", display_name=\"Top K\", info=\"Limits token selection to top K. (Default: 40)\", advanced=True\n        ),\n        FloatInput(name=\"top_p\", display_name=\"Top P\", info=\"Works together with top-k. (Default: 0.9)\", advanced=True),\n        BoolInput(name=\"verbose\", display_name=\"Verbose\", info=\"Whether to print out response text.\"),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(name=\"system\", display_name=\"System\", info=\"System to use for generating text.\", advanced=True),\n        StrInput(name=\"template\", display_name=\"Template\", info=\"Template to use for generating text.\", advanced=True),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)\n        except Exception as e:\n            msg = \"Could not initialize Ollama LLM.\"\n            raise ValueError(msg) from e\n\n        return output\n",
                                "fileTypes": [],
                                "file_path": "",
                                "password": false,
                                "name": "code",
                                "advanced": true,
                                "dynamic": true,
                                "info": "",
                                "load_from_db": false,
                                "title_case": false
                            },
                            "format": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "format",
                                "value": "",
                                "display_name": "Format",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Specify the format of the output (e.g., json).",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "input_value": {
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "input_value",
                                "value": "",
                                "display_name": "Input",
                                "advanced": false,
                                "input_types": [
                                    "Message"
                                ],
                                "dynamic": false,
                                "info": "",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "MessageInput"
                            },
                            "metadata": {
                                "trace_as_input": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "metadata",
                                "value": {},
                                "display_name": "Metadata",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Metadata to add to the run trace.",
                                "title_case": false,
                                "type": "dict",
                                "_input_type": "DictInput"
                            },
                            "mirostat": {
                                "trace_as_metadata": true,
                                "options": [
                                    "Disabled",
                                    "Mirostat",
                                    "Mirostat 2.0"
                                ],
                                "combobox": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "mirostat",
                                "value": "Disabled",
                                "display_name": "Mirostat",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Enable/disable Mirostat sampling for controlling perplexity.",
                                "real_time_refresh": true,
                                "title_case": false,
                                "type": "str",
                                "_input_type": "DropdownInput"
                            },
                            "mirostat_eta": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "mirostat_eta",
                                "value": "",
                                "display_name": "Mirostat Eta",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Learning rate for Mirostat algorithm. (Default: 0.1)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "mirostat_tau": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "mirostat_tau",
                                "value": "",
                                "display_name": "Mirostat Tau",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Controls the balance between coherence and diversity of the output. (Default: 5.0)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "model_name": {
                                "trace_as_metadata": true,
                                "options": [
                                    "gemma2:2b"
                                ],
                                "combobox": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "model_name",
                                "value": "gemma2:2b",
                                "display_name": "Model Name",
                                "advanced": false,
                                "dynamic": false,
                                "info": "Refer to https://ollama.com/library for more models.",
                                "refresh_button": true,
                                "title_case": false,
                                "type": "str",
                                "_input_type": "DropdownInput"
                            },
                            "num_ctx": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "num_ctx",
                                "value": "",
                                "display_name": "Context Window Size",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Size of the context window for generating tokens. (Default: 2048)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "num_gpu": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "num_gpu",
                                "value": "",
                                "display_name": "Number of GPUs",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "num_thread": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "num_thread",
                                "value": "",
                                "display_name": "Number of Threads",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Number of threads to use during computation. (Default: detected for optimal performance)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "repeat_last_n": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "repeat_last_n",
                                "value": "",
                                "display_name": "Repeat Last N",
                                "advanced": true,
                                "dynamic": false,
                                "info": "How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "repeat_penalty": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "repeat_penalty",
                                "value": "",
                                "display_name": "Repeat Penalty",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Penalty for repetitions in generated text. (Default: 1.1)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "stop_tokens": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "stop_tokens",
                                "value": "",
                                "display_name": "Stop Tokens",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Comma-separated list of tokens to signal the model to stop generating text.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "stream": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "stream",
                                "value": false,
                                "display_name": "Stream",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Stream the response from the model. Streaming works only in Chat.",
                                "title_case": false,
                                "type": "bool",
                                "_input_type": "BoolInput"
                            },
                            "system": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "system",
                                "value": "",
                                "display_name": "System",
                                "advanced": true,
                                "dynamic": false,
                                "info": "System to use for generating text.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "system_message": {
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "system_message",
                                "value": "",
                                "display_name": "System Message",
                                "advanced": true,
                                "input_types": [
                                    "Message"
                                ],
                                "dynamic": false,
                                "info": "System message to pass to the model.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "MessageTextInput"
                            },
                            "tags": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "tags",
                                "value": "",
                                "display_name": "Tags",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Comma-separated list of tags to add to the run trace.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "temperature": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "temperature",
                                "value": 0.2,
                                "display_name": "Temperature",
                                "advanced": false,
                                "dynamic": false,
                                "info": "Controls the creativity of model responses.",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "template": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "template",
                                "value": "",
                                "display_name": "Template",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Template to use for generating text.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "tfs_z": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "tfs_z",
                                "value": "",
                                "display_name": "TFS Z",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Tail free sampling value. (Default: 1)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "timeout": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "timeout",
                                "value": "",
                                "display_name": "Timeout",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Timeout for the request stream.",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "top_k": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "top_k",
                                "value": "",
                                "display_name": "Top K",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Limits token selection to top K. (Default: 40)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "top_p": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "top_p",
                                "value": "",
                                "display_name": "Top P",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Works together with top-k. (Default: 0.9)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "verbose": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "verbose",
                                "value": false,
                                "display_name": "Verbose",
                                "advanced": false,
                                "dynamic": false,
                                "info": "Whether to print out response text.",
                                "title_case": false,
                                "type": "bool",
                                "_input_type": "BoolInput"
                            }
                        },
                        "description": "Generate text using Ollama Local LLMs.",
                        "icon": "Ollama",
                        "base_classes": [
                            "LanguageModel",
                            "Message"
                        ],
                        "display_name": "Ollama",
                        "documentation": "",
                        "custom_fields": {},
                        "output_types": [],
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                            {
                                "types": [
                                    "Message"
                                ],
                                "selected": "Message",
                                "name": "text_output",
                                "display_name": "Text",
                                "method": "text_response",
                                "value": "__UNDEFINED__",
                                "cache": true,
                                "required_inputs": [
                                    "input_value",
                                    "stream",
                                    "system_message"
                                ]
                            },
                            {
                                "types": [
                                    "LanguageModel"
                                ],
                                "selected": "LanguageModel",
                                "name": "model_output",
                                "display_name": "Language Model",
                                "method": "build_model",
                                "value": "__UNDEFINED__",
                                "cache": true,
                                "required_inputs": [
                                    "base_url",
                                    "format",
                                    "metadata",
                                    "mirostat",
                                    "mirostat_eta",
                                    "mirostat_tau",
                                    "model_name",
                                    "num_ctx",
                                    "num_gpu",
                                    "num_thread",
                                    "repeat_last_n",
                                    "repeat_penalty",
                                    "stop_tokens",
                                    "system",
                                    "tags",
                                    "temperature",
                                    "template",
                                    "tfs_z",
                                    "timeout",
                                    "top_k",
                                    "top_p",
                                    "verbose"
                                ]
                            }
                        ],
                        "field_order": [
                            "input_value",
                            "system_message",
                            "stream",
                            "base_url",
                            "model_name",
                            "temperature",
                            "format",
                            "metadata",
                            "mirostat",
                            "mirostat_eta",
                            "mirostat_tau",
                            "num_ctx",
                            "num_gpu",
                            "num_thread",
                            "repeat_last_n",
                            "repeat_penalty",
                            "tfs_z",
                            "timeout",
                            "top_k",
                            "top_p",
                            "verbose",
                            "tags",
                            "stop_tokens",
                            "system",
                            "template",
                            "output_parser"
                        ],
                        "beta": false,
                        "edited": false,
                        "metadata": {},
                        "lf_version": "1.0.19.post2"
                    },
                    "id": "OllamaModel-8op10"
                },
                "selected": false,
                "width": 384,
                "height": 667,
                "positionAbsolute": {
                    "x": 3248.8314917966086,
                    "y": 420.56799298236155
                },
                "dragging": false
            },
            {
                "id": "OllamaModel-62B8C",
                "type": "genericNode",
                "position": {
                    "x": 4425.900666179802,
                    "y": 1062.1768415439424
                },
                "data": {
                    "type": "OllamaModel",
                    "node": {
                        "template": {
                            "_type": "Component",
                            "output_parser": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "output_parser",
                                "value": "",
                                "display_name": "Output Parser",
                                "advanced": true,
                                "input_types": [
                                    "OutputParser"
                                ],
                                "dynamic": false,
                                "info": "The parser to use to parse the output of the model",
                                "title_case": false,
                                "type": "other",
                                "_input_type": "HandleInput"
                            },
                            "base_url": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "base_url",
                                "value": "http://ollama:11434",
                                "display_name": "Base URL",
                                "advanced": false,
                                "dynamic": false,
                                "info": "Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "code": {
                                "type": "code",
                                "required": true,
                                "placeholder": "",
                                "list": false,
                                "show": true,
                                "multiline": true,
                                "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs.inputs import HandleInput\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value)\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/api/tags\")\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"name\"] for model in data.get(\"models\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure Ollama is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"llama3.1\",\n            info=\"Refer to https://ollama.com/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\", display_name=\"Format\", info=\"Specify the format of the output (e.g., json).\", advanced=True\n        ),\n        DictInput(name=\"metadata\", display_name=\"Metadata\", info=\"Metadata to add to the run trace.\", advanced=True),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(name=\"tfs_z\", display_name=\"TFS Z\", info=\"Tail free sampling value. (Default: 1)\", advanced=True),\n        IntInput(name=\"timeout\", display_name=\"Timeout\", info=\"Timeout for the request stream.\", advanced=True),\n        IntInput(\n            name=\"top_k\", display_name=\"Top K\", info=\"Limits token selection to top K. (Default: 40)\", advanced=True\n        ),\n        FloatInput(name=\"top_p\", display_name=\"Top P\", info=\"Works together with top-k. (Default: 0.9)\", advanced=True),\n        BoolInput(name=\"verbose\", display_name=\"Verbose\", info=\"Whether to print out response text.\"),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(name=\"system\", display_name=\"System\", info=\"System to use for generating text.\", advanced=True),\n        StrInput(name=\"template\", display_name=\"Template\", info=\"Template to use for generating text.\", advanced=True),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)\n        except Exception as e:\n            msg = \"Could not initialize Ollama LLM.\"\n            raise ValueError(msg) from e\n\n        return output\n",
                                "fileTypes": [],
                                "file_path": "",
                                "password": false,
                                "name": "code",
                                "advanced": true,
                                "dynamic": true,
                                "info": "",
                                "load_from_db": false,
                                "title_case": false
                            },
                            "format": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "format",
                                "value": "",
                                "display_name": "Format",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Specify the format of the output (e.g., json).",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "input_value": {
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "input_value",
                                "value": "",
                                "display_name": "Input",
                                "advanced": false,
                                "input_types": [
                                    "Message"
                                ],
                                "dynamic": false,
                                "info": "",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "MessageInput"
                            },
                            "metadata": {
                                "trace_as_input": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "metadata",
                                "value": {},
                                "display_name": "Metadata",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Metadata to add to the run trace.",
                                "title_case": false,
                                "type": "dict",
                                "_input_type": "DictInput"
                            },
                            "mirostat": {
                                "trace_as_metadata": true,
                                "options": [
                                    "Disabled",
                                    "Mirostat",
                                    "Mirostat 2.0"
                                ],
                                "combobox": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "mirostat",
                                "value": "Disabled",
                                "display_name": "Mirostat",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Enable/disable Mirostat sampling for controlling perplexity.",
                                "real_time_refresh": true,
                                "title_case": false,
                                "type": "str",
                                "_input_type": "DropdownInput"
                            },
                            "mirostat_eta": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "mirostat_eta",
                                "value": "",
                                "display_name": "Mirostat Eta",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Learning rate for Mirostat algorithm. (Default: 0.1)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "mirostat_tau": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "mirostat_tau",
                                "value": "",
                                "display_name": "Mirostat Tau",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Controls the balance between coherence and diversity of the output. (Default: 5.0)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "model_name": {
                                "trace_as_metadata": true,
                                "options": [
                                    "gemma2:2b"
                                ],
                                "combobox": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "model_name",
                                "value": "gemma2:2b",
                                "display_name": "Model Name",
                                "advanced": false,
                                "dynamic": false,
                                "info": "Refer to https://ollama.com/library for more models.",
                                "refresh_button": true,
                                "title_case": false,
                                "type": "str",
                                "_input_type": "DropdownInput"
                            },
                            "num_ctx": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "num_ctx",
                                "value": "",
                                "display_name": "Context Window Size",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Size of the context window for generating tokens. (Default: 2048)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "num_gpu": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "num_gpu",
                                "value": "",
                                "display_name": "Number of GPUs",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "num_thread": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "num_thread",
                                "value": "",
                                "display_name": "Number of Threads",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Number of threads to use during computation. (Default: detected for optimal performance)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "repeat_last_n": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "repeat_last_n",
                                "value": "",
                                "display_name": "Repeat Last N",
                                "advanced": true,
                                "dynamic": false,
                                "info": "How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "repeat_penalty": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "repeat_penalty",
                                "value": "",
                                "display_name": "Repeat Penalty",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Penalty for repetitions in generated text. (Default: 1.1)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "stop_tokens": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "stop_tokens",
                                "value": "",
                                "display_name": "Stop Tokens",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Comma-separated list of tokens to signal the model to stop generating text.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "stream": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "stream",
                                "value": false,
                                "display_name": "Stream",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Stream the response from the model. Streaming works only in Chat.",
                                "title_case": false,
                                "type": "bool",
                                "_input_type": "BoolInput"
                            },
                            "system": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "system",
                                "value": "",
                                "display_name": "System",
                                "advanced": true,
                                "dynamic": false,
                                "info": "System to use for generating text.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "system_message": {
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "system_message",
                                "value": "",
                                "display_name": "System Message",
                                "advanced": true,
                                "input_types": [
                                    "Message"
                                ],
                                "dynamic": false,
                                "info": "System message to pass to the model.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "MessageTextInput"
                            },
                            "tags": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "tags",
                                "value": "",
                                "display_name": "Tags",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Comma-separated list of tags to add to the run trace.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "temperature": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "temperature",
                                "value": 0.2,
                                "display_name": "Temperature",
                                "advanced": false,
                                "dynamic": false,
                                "info": "Controls the creativity of model responses.",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "template": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "template",
                                "value": "",
                                "display_name": "Template",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Template to use for generating text.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "tfs_z": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "tfs_z",
                                "value": "",
                                "display_name": "TFS Z",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Tail free sampling value. (Default: 1)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "timeout": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "timeout",
                                "value": "",
                                "display_name": "Timeout",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Timeout for the request stream.",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "top_k": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "top_k",
                                "value": "",
                                "display_name": "Top K",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Limits token selection to top K. (Default: 40)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "top_p": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "top_p",
                                "value": "",
                                "display_name": "Top P",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Works together with top-k. (Default: 0.9)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "verbose": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "verbose",
                                "value": false,
                                "display_name": "Verbose",
                                "advanced": false,
                                "dynamic": false,
                                "info": "Whether to print out response text.",
                                "title_case": false,
                                "type": "bool",
                                "_input_type": "BoolInput"
                            }
                        },
                        "description": "Generate text using Ollama Local LLMs.",
                        "icon": "Ollama",
                        "base_classes": [
                            "LanguageModel",
                            "Message"
                        ],
                        "display_name": "Ollama",
                        "documentation": "",
                        "custom_fields": {},
                        "output_types": [],
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                            {
                                "types": [
                                    "Message"
                                ],
                                "selected": "Message",
                                "name": "text_output",
                                "display_name": "Text",
                                "method": "text_response",
                                "value": "__UNDEFINED__",
                                "cache": true,
                                "required_inputs": [
                                    "input_value",
                                    "stream",
                                    "system_message"
                                ]
                            },
                            {
                                "types": [
                                    "LanguageModel"
                                ],
                                "selected": "LanguageModel",
                                "name": "model_output",
                                "display_name": "Language Model",
                                "method": "build_model",
                                "value": "__UNDEFINED__",
                                "cache": true,
                                "required_inputs": [
                                    "base_url",
                                    "format",
                                    "metadata",
                                    "mirostat",
                                    "mirostat_eta",
                                    "mirostat_tau",
                                    "model_name",
                                    "num_ctx",
                                    "num_gpu",
                                    "num_thread",
                                    "repeat_last_n",
                                    "repeat_penalty",
                                    "stop_tokens",
                                    "system",
                                    "tags",
                                    "temperature",
                                    "template",
                                    "tfs_z",
                                    "timeout",
                                    "top_k",
                                    "top_p",
                                    "verbose"
                                ]
                            }
                        ],
                        "field_order": [
                            "input_value",
                            "system_message",
                            "stream",
                            "base_url",
                            "model_name",
                            "temperature",
                            "format",
                            "metadata",
                            "mirostat",
                            "mirostat_eta",
                            "mirostat_tau",
                            "num_ctx",
                            "num_gpu",
                            "num_thread",
                            "repeat_last_n",
                            "repeat_penalty",
                            "tfs_z",
                            "timeout",
                            "top_k",
                            "top_p",
                            "verbose",
                            "tags",
                            "stop_tokens",
                            "system",
                            "template",
                            "output_parser"
                        ],
                        "beta": false,
                        "edited": false,
                        "metadata": {},
                        "lf_version": "1.0.19.post2"
                    },
                    "id": "OllamaModel-62B8C"
                },
                "selected": false,
                "width": 384,
                "height": 667,
                "positionAbsolute": {
                    "x": 4425.900666179802,
                    "y": 1062.1768415439424
                },
                "dragging": false
            },
            {
                "id": "OllamaModel-fBp6P",
                "type": "genericNode",
                "position": {
                    "x": 5475.591613275037,
                    "y": 45.50987930114357
                },
                "data": {
                    "type": "OllamaModel",
                    "node": {
                        "template": {
                            "_type": "Component",
                            "output_parser": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "output_parser",
                                "value": "",
                                "display_name": "Output Parser",
                                "advanced": true,
                                "input_types": [
                                    "OutputParser"
                                ],
                                "dynamic": false,
                                "info": "The parser to use to parse the output of the model",
                                "title_case": false,
                                "type": "other",
                                "_input_type": "HandleInput"
                            },
                            "base_url": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "base_url",
                                "value": "http://ollama:11434",
                                "display_name": "Base URL",
                                "advanced": false,
                                "dynamic": false,
                                "info": "Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "code": {
                                "type": "code",
                                "required": true,
                                "placeholder": "",
                                "list": false,
                                "show": true,
                                "multiline": true,
                                "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs.inputs import HandleInput\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value)\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/api/tags\")\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"name\"] for model in data.get(\"models\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure Ollama is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"llama3.1\",\n            info=\"Refer to https://ollama.com/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\", display_name=\"Format\", info=\"Specify the format of the output (e.g., json).\", advanced=True\n        ),\n        DictInput(name=\"metadata\", display_name=\"Metadata\", info=\"Metadata to add to the run trace.\", advanced=True),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(name=\"tfs_z\", display_name=\"TFS Z\", info=\"Tail free sampling value. (Default: 1)\", advanced=True),\n        IntInput(name=\"timeout\", display_name=\"Timeout\", info=\"Timeout for the request stream.\", advanced=True),\n        IntInput(\n            name=\"top_k\", display_name=\"Top K\", info=\"Limits token selection to top K. (Default: 40)\", advanced=True\n        ),\n        FloatInput(name=\"top_p\", display_name=\"Top P\", info=\"Works together with top-k. (Default: 0.9)\", advanced=True),\n        BoolInput(name=\"verbose\", display_name=\"Verbose\", info=\"Whether to print out response text.\"),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(name=\"system\", display_name=\"System\", info=\"System to use for generating text.\", advanced=True),\n        StrInput(name=\"template\", display_name=\"Template\", info=\"Template to use for generating text.\", advanced=True),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)\n        except Exception as e:\n            msg = \"Could not initialize Ollama LLM.\"\n            raise ValueError(msg) from e\n\n        return output\n",
                                "fileTypes": [],
                                "file_path": "",
                                "password": false,
                                "name": "code",
                                "advanced": true,
                                "dynamic": true,
                                "info": "",
                                "load_from_db": false,
                                "title_case": false
                            },
                            "format": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "format",
                                "value": "",
                                "display_name": "Format",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Specify the format of the output (e.g., json).",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "input_value": {
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "input_value",
                                "value": "",
                                "display_name": "Input",
                                "advanced": false,
                                "input_types": [
                                    "Message"
                                ],
                                "dynamic": false,
                                "info": "",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "MessageInput"
                            },
                            "metadata": {
                                "trace_as_input": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "metadata",
                                "value": {},
                                "display_name": "Metadata",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Metadata to add to the run trace.",
                                "title_case": false,
                                "type": "dict",
                                "_input_type": "DictInput"
                            },
                            "mirostat": {
                                "trace_as_metadata": true,
                                "options": [
                                    "Disabled",
                                    "Mirostat",
                                    "Mirostat 2.0"
                                ],
                                "combobox": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "mirostat",
                                "value": "Disabled",
                                "display_name": "Mirostat",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Enable/disable Mirostat sampling for controlling perplexity.",
                                "real_time_refresh": true,
                                "title_case": false,
                                "type": "str",
                                "_input_type": "DropdownInput"
                            },
                            "mirostat_eta": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "mirostat_eta",
                                "value": "",
                                "display_name": "Mirostat Eta",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Learning rate for Mirostat algorithm. (Default: 0.1)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "mirostat_tau": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "mirostat_tau",
                                "value": "",
                                "display_name": "Mirostat Tau",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Controls the balance between coherence and diversity of the output. (Default: 5.0)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "model_name": {
                                "trace_as_metadata": true,
                                "options": [
                                    "gemma2:2b"
                                ],
                                "combobox": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "model_name",
                                "value": "gemma2:2b",
                                "display_name": "Model Name",
                                "advanced": false,
                                "dynamic": false,
                                "info": "Refer to https://ollama.com/library for more models.",
                                "refresh_button": true,
                                "title_case": false,
                                "type": "str",
                                "_input_type": "DropdownInput"
                            },
                            "num_ctx": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "num_ctx",
                                "value": "",
                                "display_name": "Context Window Size",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Size of the context window for generating tokens. (Default: 2048)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "num_gpu": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "num_gpu",
                                "value": "",
                                "display_name": "Number of GPUs",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "num_thread": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "num_thread",
                                "value": "",
                                "display_name": "Number of Threads",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Number of threads to use during computation. (Default: detected for optimal performance)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "repeat_last_n": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "repeat_last_n",
                                "value": "",
                                "display_name": "Repeat Last N",
                                "advanced": true,
                                "dynamic": false,
                                "info": "How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "repeat_penalty": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "repeat_penalty",
                                "value": "",
                                "display_name": "Repeat Penalty",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Penalty for repetitions in generated text. (Default: 1.1)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "stop_tokens": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "stop_tokens",
                                "value": "",
                                "display_name": "Stop Tokens",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Comma-separated list of tokens to signal the model to stop generating text.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "stream": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "stream",
                                "value": false,
                                "display_name": "Stream",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Stream the response from the model. Streaming works only in Chat.",
                                "title_case": false,
                                "type": "bool",
                                "_input_type": "BoolInput"
                            },
                            "system": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "system",
                                "value": "",
                                "display_name": "System",
                                "advanced": true,
                                "dynamic": false,
                                "info": "System to use for generating text.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "system_message": {
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "system_message",
                                "value": "",
                                "display_name": "System Message",
                                "advanced": true,
                                "input_types": [
                                    "Message"
                                ],
                                "dynamic": false,
                                "info": "System message to pass to the model.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "MessageTextInput"
                            },
                            "tags": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "tags",
                                "value": "",
                                "display_name": "Tags",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Comma-separated list of tags to add to the run trace.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "temperature": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "temperature",
                                "value": 0.2,
                                "display_name": "Temperature",
                                "advanced": false,
                                "dynamic": false,
                                "info": "Controls the creativity of model responses.",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "template": {
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "template",
                                "value": "",
                                "display_name": "Template",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Template to use for generating text.",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "StrInput"
                            },
                            "tfs_z": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "tfs_z",
                                "value": "",
                                "display_name": "TFS Z",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Tail free sampling value. (Default: 1)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "timeout": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "timeout",
                                "value": "",
                                "display_name": "Timeout",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Timeout for the request stream.",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "top_k": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "top_k",
                                "value": "",
                                "display_name": "Top K",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Limits token selection to top K. (Default: 40)",
                                "title_case": false,
                                "type": "int",
                                "_input_type": "IntInput"
                            },
                            "top_p": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "top_p",
                                "value": "",
                                "display_name": "Top P",
                                "advanced": true,
                                "dynamic": false,
                                "info": "Works together with top-k. (Default: 0.9)",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            },
                            "verbose": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "verbose",
                                "value": false,
                                "display_name": "Verbose",
                                "advanced": false,
                                "dynamic": false,
                                "info": "Whether to print out response text.",
                                "title_case": false,
                                "type": "bool",
                                "_input_type": "BoolInput"
                            }
                        },
                        "description": "Generate text using Ollama Local LLMs.",
                        "icon": "Ollama",
                        "base_classes": [
                            "LanguageModel",
                            "Message"
                        ],
                        "display_name": "Ollama",
                        "documentation": "",
                        "custom_fields": {},
                        "output_types": [],
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                            {
                                "types": [
                                    "Message"
                                ],
                                "selected": "Message",
                                "name": "text_output",
                                "display_name": "Text",
                                "method": "text_response",
                                "value": "__UNDEFINED__",
                                "cache": true,
                                "required_inputs": [
                                    "input_value",
                                    "stream",
                                    "system_message"
                                ]
                            },
                            {
                                "types": [
                                    "LanguageModel"
                                ],
                                "selected": "LanguageModel",
                                "name": "model_output",
                                "display_name": "Language Model",
                                "method": "build_model",
                                "value": "__UNDEFINED__",
                                "cache": true,
                                "required_inputs": [
                                    "base_url",
                                    "format",
                                    "metadata",
                                    "mirostat",
                                    "mirostat_eta",
                                    "mirostat_tau",
                                    "model_name",
                                    "num_ctx",
                                    "num_gpu",
                                    "num_thread",
                                    "repeat_last_n",
                                    "repeat_penalty",
                                    "stop_tokens",
                                    "system",
                                    "tags",
                                    "temperature",
                                    "template",
                                    "tfs_z",
                                    "timeout",
                                    "top_k",
                                    "top_p",
                                    "verbose"
                                ]
                            }
                        ],
                        "field_order": [
                            "input_value",
                            "system_message",
                            "stream",
                            "base_url",
                            "model_name",
                            "temperature",
                            "format",
                            "metadata",
                            "mirostat",
                            "mirostat_eta",
                            "mirostat_tau",
                            "num_ctx",
                            "num_gpu",
                            "num_thread",
                            "repeat_last_n",
                            "repeat_penalty",
                            "tfs_z",
                            "timeout",
                            "top_k",
                            "top_p",
                            "verbose",
                            "tags",
                            "stop_tokens",
                            "system",
                            "template",
                            "output_parser"
                        ],
                        "beta": false,
                        "edited": false,
                        "metadata": {},
                        "lf_version": "1.0.19.post2"
                    },
                    "id": "OllamaModel-fBp6P"
                },
                "selected": false,
                "width": 384,
                "height": 667,
                "positionAbsolute": {
                    "x": 5475.591613275037,
                    "y": 45.50987930114357
                },
                "dragging": false
            },
            {
                "id": "OllamaEmbeddings-KdioM",
                "type": "genericNode",
                "position": {
                    "x": 158.73005203311266,
                    "y": 302.6251899379537
                },
                "data": {
                    "type": "OllamaEmbeddings",
                    "node": {
                        "template": {
                            "_type": "Component",
                            "base_url": {
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "base_url",
                                "value": "http://localhost:11434",
                                "display_name": "Ollama Base URL",
                                "advanced": false,
                                "input_types": [
                                    "Message"
                                ],
                                "dynamic": false,
                                "info": "",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "MessageTextInput"
                            },
                            "code": {
                                "type": "code",
                                "required": true,
                                "placeholder": "",
                                "list": false,
                                "show": true,
                                "multiline": true,
                                "value": "from langchain_community.embeddings import OllamaEmbeddings\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import FloatInput, MessageTextInput, Output\n\n\nclass OllamaEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Ollama Embeddings\"\n    description: str = \"Generate embeddings using Ollama models.\"\n    documentation = \"https://python.langchain.com/docs/integrations/text_embedding/ollama\"\n    icon = \"Ollama\"\n    name = \"OllamaEmbeddings\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"model\",\n            display_name=\"Ollama Model\",\n            value=\"nomic-embed-text\",\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Ollama Base URL\",\n            value=\"http://localhost:11434\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            output = OllamaEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n                temperature=self.temperature,\n            )\n        except Exception as e:\n            msg = \"Could not connect to Ollama API.\"\n            raise ValueError(msg) from e\n        return output\n",
                                "fileTypes": [],
                                "file_path": "",
                                "password": false,
                                "name": "code",
                                "advanced": true,
                                "dynamic": true,
                                "info": "",
                                "load_from_db": false,
                                "title_case": false
                            },
                            "model": {
                                "trace_as_input": true,
                                "trace_as_metadata": true,
                                "load_from_db": false,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "model",
                                "value": "llama3.1",
                                "display_name": "Ollama Model",
                                "advanced": false,
                                "input_types": [
                                    "Message"
                                ],
                                "dynamic": false,
                                "info": "",
                                "title_case": false,
                                "type": "str",
                                "_input_type": "MessageTextInput"
                            },
                            "temperature": {
                                "trace_as_metadata": true,
                                "list": false,
                                "required": false,
                                "placeholder": "",
                                "show": true,
                                "name": "temperature",
                                "value": 0.1,
                                "display_name": "Model Temperature",
                                "advanced": true,
                                "dynamic": false,
                                "info": "",
                                "title_case": false,
                                "type": "float",
                                "_input_type": "FloatInput"
                            }
                        },
                        "description": "Generate embeddings using Ollama models.",
                        "icon": "Ollama",
                        "base_classes": [
                            "Embeddings"
                        ],
                        "display_name": "Ollama Embeddings",
                        "documentation": "https://python.langchain.com/docs/integrations/text_embedding/ollama",
                        "custom_fields": {},
                        "output_types": [],
                        "pinned": false,
                        "conditional_paths": [],
                        "frozen": false,
                        "outputs": [
                            {
                                "types": [
                                    "Embeddings"
                                ],
                                "selected": "Embeddings",
                                "name": "embeddings",
                                "display_name": "Embeddings",
                                "method": "build_embeddings",
                                "value": "__UNDEFINED__",
                                "cache": true
                            }
                        ],
                        "field_order": [
                            "model",
                            "base_url",
                            "temperature"
                        ],
                        "beta": false,
                        "edited": true,
                        "metadata": {}
                    },
                    "id": "OllamaEmbeddings-KdioM"
                },
                "selected": false,
                "width": 384,
                "height": 383,
                "positionAbsolute": {
                    "x": 158.73005203311266,
                    "y": 302.6251899379537
                },
                "dragging": false
            }
        ],
        "edges": [
            {
                "className": "",
                "data": {
                    "sourceHandle": {
                        "dataType": "ParseData",
                        "id": "ParseData-qNt15",
                        "name": "text",
                        "output_types": [
                            "Message"
                        ]
                    },
                    "targetHandle": {
                        "fieldName": "context",
                        "id": "Prompt-KEwey",
                        "inputTypes": [
                            "Message",
                            "Text"
                        ],
                        "type": "str"
                    }
                },
                "id": "reactflow__edge-ParseData-qNt15{œdataTypeœ:œParseDataœ,œidœ:œParseData-qNt15œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-KEwey{œfieldNameœ:œcontextœ,œidœ:œPrompt-KEweyœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
                "source": "ParseData-qNt15",
                "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-qNt15œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
                "target": "Prompt-KEwey",
                "targetHandle": "{œfieldNameœ:œcontextœ,œidœ:œPrompt-KEweyœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
                "animated": false
            },
            {
                "className": "",
                "data": {
                    "sourceHandle": {
                        "dataType": "ChatInput",
                        "id": "ChatInput-0PoBn",
                        "name": "message",
                        "output_types": [
                            "Message"
                        ]
                    },
                    "targetHandle": {
                        "fieldName": "question",
                        "id": "Prompt-KEwey",
                        "inputTypes": [
                            "Message",
                            "Text"
                        ],
                        "type": "str"
                    }
                },
                "id": "reactflow__edge-ChatInput-0PoBn{œdataTypeœ:œChatInputœ,œidœ:œChatInput-0PoBnœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-KEwey{œfieldNameœ:œquestionœ,œidœ:œPrompt-KEweyœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
                "source": "ChatInput-0PoBn",
                "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-0PoBnœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
                "target": "Prompt-KEwey",
                "targetHandle": "{œfieldNameœ:œquestionœ,œidœ:œPrompt-KEweyœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
                "animated": false
            },
            {
                "source": "MultiQueryRetriever-DEUr2",
                "sourceHandle": "{œdataTypeœ:œMultiQueryRetrieverœ,œidœ:œMultiQueryRetriever-DEUr2œ,œnameœ:œdocumentsœ,œoutput_typesœ:[œDataœ]}",
                "target": "ParseData-qNt15",
                "targetHandle": "{œfieldNameœ:œdataœ,œidœ:œParseData-qNt15œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
                "data": {
                    "targetHandle": {
                        "fieldName": "data",
                        "id": "ParseData-qNt15",
                        "inputTypes": [
                            "Data"
                        ],
                        "type": "other"
                    },
                    "sourceHandle": {
                        "dataType": "MultiQueryRetriever",
                        "id": "MultiQueryRetriever-DEUr2",
                        "name": "documents",
                        "output_types": [
                            "Data"
                        ]
                    }
                },
                "id": "reactflow__edge-MultiQueryRetriever-DEUr2{œdataTypeœ:œMultiQueryRetrieverœ,œidœ:œMultiQueryRetriever-DEUr2œ,œnameœ:œdocumentsœ,œoutput_typesœ:[œDataœ]}-ParseData-qNt15{œfieldNameœ:œdataœ,œidœ:œParseData-qNt15œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
                "animated": false,
                "className": ""
            },
            {
                "source": "ChatInput-0PoBn",
                "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-0PoBnœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
                "target": "Prompt-k5TEn",
                "targetHandle": "{œfieldNameœ:œquestionœ,œidœ:œPrompt-k5TEnœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
                "data": {
                    "targetHandle": {
                        "fieldName": "question",
                        "id": "Prompt-k5TEn",
                        "inputTypes": [
                            "Message",
                            "Text"
                        ],
                        "type": "str"
                    },
                    "sourceHandle": {
                        "dataType": "ChatInput",
                        "id": "ChatInput-0PoBn",
                        "name": "message",
                        "output_types": [
                            "Message"
                        ]
                    }
                },
                "id": "reactflow__edge-ChatInput-0PoBn{œdataTypeœ:œChatInputœ,œidœ:œChatInput-0PoBnœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-k5TEn{œfieldNameœ:œquestionœ,œidœ:œPrompt-k5TEnœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
                "animated": false,
                "className": ""
            },
            {
                "source": "ChatInput-0PoBn",
                "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-0PoBnœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
                "target": "MultiQueryRetriever-DEUr2",
                "targetHandle": "{œfieldNameœ:œsearch_queryœ,œidœ:œMultiQueryRetriever-DEUr2œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
                "data": {
                    "targetHandle": {
                        "fieldName": "search_query",
                        "id": "MultiQueryRetriever-DEUr2",
                        "inputTypes": [
                            "Message",
                            "Text"
                        ],
                        "type": "str"
                    },
                    "sourceHandle": {
                        "dataType": "ChatInput",
                        "id": "ChatInput-0PoBn",
                        "name": "message",
                        "output_types": [
                            "Message"
                        ]
                    }
                },
                "id": "reactflow__edge-ChatInput-0PoBn{œdataTypeœ:œChatInputœ,œidœ:œChatInput-0PoBnœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-MultiQueryRetriever-DEUr2{œfieldNameœ:œsearch_queryœ,œidœ:œMultiQueryRetriever-DEUr2œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
                "animated": false,
                "className": ""
            },
            {
                "source": "Prompt-b8sx9",
                "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-b8sx9œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
                "target": "MultiQueryRetriever-jZ5YG",
                "targetHandle": "{œfieldNameœ:œpromptœ,œidœ:œMultiQueryRetriever-jZ5YGœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
                "data": {
                    "targetHandle": {
                        "fieldName": "prompt",
                        "id": "MultiQueryRetriever-jZ5YG",
                        "inputTypes": [
                            "Message"
                        ],
                        "type": "str"
                    },
                    "sourceHandle": {
                        "dataType": "Prompt",
                        "id": "Prompt-b8sx9",
                        "name": "prompt",
                        "output_types": [
                            "Message"
                        ]
                    }
                },
                "id": "reactflow__edge-Prompt-b8sx9{œdataTypeœ:œPromptœ,œidœ:œPrompt-b8sx9œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-MultiQueryRetriever-jZ5YG{œfieldNameœ:œpromptœ,œidœ:œMultiQueryRetriever-jZ5YGœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
                "animated": false,
                "className": ""
            },
            {
                "source": "AmazonKendra-hgFr5",
                "sourceHandle": "{œdataTypeœ:œTavilyœ,œidœ:œAmazonKendra-hgFr5œ,œnameœ:œbase_retrieverœ,œoutput_typesœ:[œRetrieverœ]}",
                "target": "MultiQueryRetriever-jZ5YG",
                "targetHandle": "{œfieldNameœ:œretrieverœ,œidœ:œMultiQueryRetriever-jZ5YGœ,œinputTypesœ:[œRetrieverœ],œtypeœ:œotherœ}",
                "data": {
                    "targetHandle": {
                        "fieldName": "retriever",
                        "id": "MultiQueryRetriever-jZ5YG",
                        "inputTypes": [
                            "Retriever"
                        ],
                        "type": "other"
                    },
                    "sourceHandle": {
                        "dataType": "Tavily",
                        "id": "AmazonKendra-hgFr5",
                        "name": "base_retriever",
                        "output_types": [
                            "Retriever"
                        ]
                    }
                },
                "id": "reactflow__edge-AmazonKendra-hgFr5{œdataTypeœ:œTavilyœ,œidœ:œAmazonKendra-hgFr5œ,œnameœ:œbase_retrieverœ,œoutput_typesœ:[œRetrieverœ]}-MultiQueryRetriever-jZ5YG{œfieldNameœ:œretrieverœ,œidœ:œMultiQueryRetriever-jZ5YGœ,œinputTypesœ:[œRetrieverœ],œtypeœ:œotherœ}",
                "animated": false,
                "className": ""
            },
            {
                "source": "ChatInput-0PoBn",
                "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-0PoBnœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
                "target": "MultiQueryRetriever-jZ5YG",
                "targetHandle": "{œfieldNameœ:œsearch_queryœ,œidœ:œMultiQueryRetriever-jZ5YGœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
                "data": {
                    "targetHandle": {
                        "fieldName": "search_query",
                        "id": "MultiQueryRetriever-jZ5YG",
                        "inputTypes": [
                            "Message",
                            "Text"
                        ],
                        "type": "str"
                    },
                    "sourceHandle": {
                        "dataType": "ChatInput",
                        "id": "ChatInput-0PoBn",
                        "name": "message",
                        "output_types": [
                            "Message"
                        ]
                    }
                },
                "id": "reactflow__edge-ChatInput-0PoBn{œdataTypeœ:œChatInputœ,œidœ:œChatInput-0PoBnœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-MultiQueryRetriever-jZ5YG{œfieldNameœ:œsearch_queryœ,œidœ:œMultiQueryRetriever-jZ5YGœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
                "animated": false,
                "className": ""
            },
            {
                "source": "ParseData-Sdfos",
                "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-Sdfosœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
                "target": "Prompt-KEwey",
                "targetHandle": "{œfieldNameœ:œinsightsœ,œidœ:œPrompt-KEweyœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
                "data": {
                    "targetHandle": {
                        "fieldName": "insights",
                        "id": "Prompt-KEwey",
                        "inputTypes": [
                            "Message",
                            "Text"
                        ],
                        "type": "str"
                    },
                    "sourceHandle": {
                        "dataType": "ParseData",
                        "id": "ParseData-Sdfos",
                        "name": "text",
                        "output_types": [
                            "Message"
                        ]
                    }
                },
                "id": "reactflow__edge-ParseData-Sdfos{œdataTypeœ:œParseDataœ,œidœ:œParseData-Sdfosœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-KEwey{œfieldNameœ:œinsightsœ,œidœ:œPrompt-KEweyœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
                "animated": false,
                "className": "",
                "selected": false
            },
            {
                "source": "ChatInput-0PoBn",
                "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-0PoBnœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
                "target": "Prompt-JKqdq",
                "targetHandle": "{œfieldNameœ:œquestionœ,œidœ:œPrompt-JKqdqœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
                "data": {
                    "targetHandle": {
                        "fieldName": "question",
                        "id": "Prompt-JKqdq",
                        "inputTypes": [
                            "Message",
                            "Text"
                        ],
                        "type": "str"
                    },
                    "sourceHandle": {
                        "dataType": "ChatInput",
                        "id": "ChatInput-0PoBn",
                        "name": "message",
                        "output_types": [
                            "Message"
                        ]
                    }
                },
                "id": "reactflow__edge-ChatInput-0PoBn{œdataTypeœ:œChatInputœ,œidœ:œChatInput-0PoBnœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-JKqdq{œfieldNameœ:œquestionœ,œidœ:œPrompt-JKqdqœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
                "animated": false,
                "className": "",
                "selected": false
            },
            {
                "source": "ParseData-qNt15",
                "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-qNt15œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
                "target": "Prompt-JKqdq",
                "targetHandle": "{œfieldNameœ:œcontextœ,œidœ:œPrompt-JKqdqœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
                "data": {
                    "targetHandle": {
                        "fieldName": "context",
                        "id": "Prompt-JKqdq",
                        "inputTypes": [
                            "Message",
                            "Text"
                        ],
                        "type": "str"
                    },
                    "sourceHandle": {
                        "dataType": "ParseData",
                        "id": "ParseData-qNt15",
                        "name": "text",
                        "output_types": [
                            "Message"
                        ]
                    }
                },
                "id": "reactflow__edge-ParseData-qNt15{œdataTypeœ:œParseDataœ,œidœ:œParseData-qNt15œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-JKqdq{œfieldNameœ:œcontextœ,œidœ:œPrompt-JKqdqœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
                "animated": false,
                "className": ""
            },
            {
                "source": "ParseData-Sdfos",
                "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-Sdfosœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
                "target": "Prompt-JKqdq",
                "targetHandle": "{œfieldNameœ:œinsightsœ,œidœ:œPrompt-JKqdqœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
                "data": {
                    "targetHandle": {
                        "fieldName": "insights",
                        "id": "Prompt-JKqdq",
                        "inputTypes": [
                            "Message",
                            "Text"
                        ],
                        "type": "str"
                    },
                    "sourceHandle": {
                        "dataType": "ParseData",
                        "id": "ParseData-Sdfos",
                        "name": "text",
                        "output_types": [
                            "Message"
                        ]
                    }
                },
                "id": "reactflow__edge-ParseData-Sdfos{œdataTypeœ:œParseDataœ,œidœ:œParseData-Sdfosœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-JKqdq{œfieldNameœ:œinsightsœ,œidœ:œPrompt-JKqdqœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
                "animated": false,
                "className": ""
            },
            {
                "source": "MultiQueryRetriever-jZ5YG",
                "sourceHandle": "{œdataTypeœ:œMultiQueryRetrieverœ,œidœ:œMultiQueryRetriever-jZ5YGœ,œnameœ:œdocumentsœ,œoutput_typesœ:[œDataœ]}",
                "target": "ParseData-Sdfos",
                "targetHandle": "{œfieldNameœ:œdataœ,œidœ:œParseData-Sdfosœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
                "data": {
                    "targetHandle": {
                        "fieldName": "data",
                        "id": "ParseData-Sdfos",
                        "inputTypes": [
                            "Data"
                        ],
                        "type": "other"
                    },
                    "sourceHandle": {
                        "dataType": "MultiQueryRetriever",
                        "id": "MultiQueryRetriever-jZ5YG",
                        "name": "documents",
                        "output_types": [
                            "Data"
                        ]
                    }
                },
                "id": "reactflow__edge-MultiQueryRetriever-jZ5YG{œdataTypeœ:œMultiQueryRetrieverœ,œidœ:œMultiQueryRetriever-jZ5YGœ,œnameœ:œdocumentsœ,œoutput_typesœ:[œDataœ]}-ParseData-Sdfos{œfieldNameœ:œdataœ,œidœ:œParseData-Sdfosœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
                "animated": false,
                "className": ""
            },
            {
                "source": "ChatInput-0PoBn",
                "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-0PoBnœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
                "target": "QdrantVectorStoreComponent-e7yEx",
                "targetHandle": "{œfieldNameœ:œsearch_queryœ,œidœ:œQdrantVectorStoreComponent-e7yExœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
                "data": {
                    "targetHandle": {
                        "fieldName": "search_query",
                        "id": "QdrantVectorStoreComponent-e7yEx",
                        "inputTypes": [
                            "Message"
                        ],
                        "type": "str"
                    },
                    "sourceHandle": {
                        "dataType": "ChatInput",
                        "id": "ChatInput-0PoBn",
                        "name": "message",
                        "output_types": [
                            "Message"
                        ]
                    }
                },
                "id": "reactflow__edge-ChatInput-0PoBn{œdataTypeœ:œChatInputœ,œidœ:œChatInput-0PoBnœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-QdrantVectorStoreComponent-e7yEx{œfieldNameœ:œsearch_queryœ,œidœ:œQdrantVectorStoreComponent-e7yExœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
                "animated": false,
                "className": ""
            },
            {
                "source": "QdrantVectorStoreComponent-e7yEx",
                "sourceHandle": "{œdataTypeœ:œQdrantVectorStoreComponentœ,œidœ:œQdrantVectorStoreComponent-e7yExœ,œnameœ:œbase_retrieverœ,œoutput_typesœ:[œRetrieverœ]}",
                "target": "MultiQueryRetriever-DEUr2",
                "targetHandle": "{œfieldNameœ:œretrieverœ,œidœ:œMultiQueryRetriever-DEUr2œ,œinputTypesœ:[œRetrieverœ],œtypeœ:œotherœ}",
                "data": {
                    "targetHandle": {
                        "fieldName": "retriever",
                        "id": "MultiQueryRetriever-DEUr2",
                        "inputTypes": [
                            "Retriever"
                        ],
                        "type": "other"
                    },
                    "sourceHandle": {
                        "dataType": "QdrantVectorStoreComponent",
                        "id": "QdrantVectorStoreComponent-e7yEx",
                        "name": "base_retriever",
                        "output_types": [
                            "Retriever"
                        ]
                    }
                },
                "id": "reactflow__edge-QdrantVectorStoreComponent-e7yEx{œdataTypeœ:œQdrantVectorStoreComponentœ,œidœ:œQdrantVectorStoreComponent-e7yExœ,œnameœ:œbase_retrieverœ,œoutput_typesœ:[œRetrieverœ]}-MultiQueryRetriever-DEUr2{œfieldNameœ:œretrieverœ,œidœ:œMultiQueryRetriever-DEUr2œ,œinputTypesœ:[œRetrieverœ],œtypeœ:œotherœ}",
                "className": "",
                "animated": false
            },
            {
                "source": "OllamaModel-d0LkQ",
                "sourceHandle": "{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-d0LkQœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
                "target": "MultiQueryRetriever-jZ5YG",
                "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œMultiQueryRetriever-jZ5YGœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
                "data": {
                    "targetHandle": {
                        "fieldName": "llm",
                        "id": "MultiQueryRetriever-jZ5YG",
                        "inputTypes": [
                            "LanguageModel"
                        ],
                        "type": "other"
                    },
                    "sourceHandle": {
                        "dataType": "OllamaModel",
                        "id": "OllamaModel-d0LkQ",
                        "name": "model_output",
                        "output_types": [
                            "LanguageModel"
                        ]
                    }
                },
                "id": "reactflow__edge-OllamaModel-d0LkQ{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-d0LkQœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-MultiQueryRetriever-jZ5YG{œfieldNameœ:œllmœ,œidœ:œMultiQueryRetriever-jZ5YGœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
                "animated": false,
                "className": ""
            },
            {
                "source": "OllamaModel-VF7Fh",
                "sourceHandle": "{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-VF7Fhœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
                "target": "MultiQueryRetriever-DEUr2",
                "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œMultiQueryRetriever-DEUr2œ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
                "data": {
                    "targetHandle": {
                        "fieldName": "llm",
                        "id": "MultiQueryRetriever-DEUr2",
                        "inputTypes": [
                            "LanguageModel"
                        ],
                        "type": "other"
                    },
                    "sourceHandle": {
                        "dataType": "OllamaModel",
                        "id": "OllamaModel-VF7Fh",
                        "name": "model_output",
                        "output_types": [
                            "LanguageModel"
                        ]
                    }
                },
                "id": "reactflow__edge-OllamaModel-VF7Fh{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-VF7Fhœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-MultiQueryRetriever-DEUr2{œfieldNameœ:œllmœ,œidœ:œMultiQueryRetriever-DEUr2œ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
                "animated": false,
                "className": ""
            },
            {
                "source": "Prompt-JKqdq",
                "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-JKqdqœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
                "target": "OllamaModel-8op10",
                "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-8op10œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
                "data": {
                    "targetHandle": {
                        "fieldName": "input_value",
                        "id": "OllamaModel-8op10",
                        "inputTypes": [
                            "Message"
                        ],
                        "type": "str"
                    },
                    "sourceHandle": {
                        "dataType": "Prompt",
                        "id": "Prompt-JKqdq",
                        "name": "prompt",
                        "output_types": [
                            "Message"
                        ]
                    }
                },
                "id": "reactflow__edge-Prompt-JKqdq{œdataTypeœ:œPromptœ,œidœ:œPrompt-JKqdqœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OllamaModel-8op10{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-8op10œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
                "animated": false,
                "className": ""
            },
            {
                "source": "OllamaModel-8op10",
                "sourceHandle": "{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-8op10œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
                "target": "Prompt-KEwey",
                "targetHandle": "{œfieldNameœ:œoutlineœ,œidœ:œPrompt-KEweyœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
                "data": {
                    "targetHandle": {
                        "fieldName": "outline",
                        "id": "Prompt-KEwey",
                        "inputTypes": [
                            "Message",
                            "Text"
                        ],
                        "type": "str"
                    },
                    "sourceHandle": {
                        "dataType": "OllamaModel",
                        "id": "OllamaModel-8op10",
                        "name": "text_output",
                        "output_types": [
                            "Message"
                        ]
                    }
                },
                "id": "reactflow__edge-OllamaModel-8op10{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-8op10œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-Prompt-KEwey{œfieldNameœ:œoutlineœ,œidœ:œPrompt-KEweyœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
                "animated": false,
                "className": ""
            },
            {
                "source": "Prompt-KEwey",
                "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-KEweyœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
                "target": "OllamaModel-62B8C",
                "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-62B8Cœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
                "data": {
                    "targetHandle": {
                        "fieldName": "input_value",
                        "id": "OllamaModel-62B8C",
                        "inputTypes": [
                            "Message"
                        ],
                        "type": "str"
                    },
                    "sourceHandle": {
                        "dataType": "Prompt",
                        "id": "Prompt-KEwey",
                        "name": "prompt",
                        "output_types": [
                            "Message"
                        ]
                    }
                },
                "id": "reactflow__edge-Prompt-KEwey{œdataTypeœ:œPromptœ,œidœ:œPrompt-KEweyœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OllamaModel-62B8C{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-62B8Cœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
                "animated": false,
                "className": ""
            },
            {
                "source": "OllamaModel-62B8C",
                "sourceHandle": "{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-62B8Cœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
                "target": "ChatOutput-VDBdC",
                "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-VDBdCœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
                "data": {
                    "targetHandle": {
                        "fieldName": "input_value",
                        "id": "ChatOutput-VDBdC",
                        "inputTypes": [
                            "Message"
                        ],
                        "type": "str"
                    },
                    "sourceHandle": {
                        "dataType": "OllamaModel",
                        "id": "OllamaModel-62B8C",
                        "name": "text_output",
                        "output_types": [
                            "Message"
                        ]
                    }
                },
                "id": "reactflow__edge-OllamaModel-62B8C{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-62B8Cœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-VDBdC{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-VDBdCœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
                "animated": false,
                "className": ""
            },
            {
                "source": "OllamaModel-62B8C",
                "sourceHandle": "{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-62B8Cœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
                "target": "Prompt-k5TEn",
                "targetHandle": "{œfieldNameœ:œresponseœ,œidœ:œPrompt-k5TEnœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
                "data": {
                    "targetHandle": {
                        "fieldName": "response",
                        "id": "Prompt-k5TEn",
                        "inputTypes": [
                            "Message",
                            "Text"
                        ],
                        "type": "str"
                    },
                    "sourceHandle": {
                        "dataType": "OllamaModel",
                        "id": "OllamaModel-62B8C",
                        "name": "text_output",
                        "output_types": [
                            "Message"
                        ]
                    }
                },
                "id": "reactflow__edge-OllamaModel-62B8C{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-62B8Cœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-Prompt-k5TEn{œfieldNameœ:œresponseœ,œidœ:œPrompt-k5TEnœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
                "animated": false,
                "className": ""
            },
            {
                "source": "Prompt-k5TEn",
                "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-k5TEnœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
                "target": "OllamaModel-fBp6P",
                "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-fBp6Pœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
                "data": {
                    "targetHandle": {
                        "fieldName": "input_value",
                        "id": "OllamaModel-fBp6P",
                        "inputTypes": [
                            "Message"
                        ],
                        "type": "str"
                    },
                    "sourceHandle": {
                        "dataType": "Prompt",
                        "id": "Prompt-k5TEn",
                        "name": "prompt",
                        "output_types": [
                            "Message"
                        ]
                    }
                },
                "id": "reactflow__edge-Prompt-k5TEn{œdataTypeœ:œPromptœ,œidœ:œPrompt-k5TEnœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OllamaModel-fBp6P{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-fBp6Pœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
                "animated": false,
                "className": ""
            },
            {
                "source": "OllamaEmbeddings-KdioM",
                "sourceHandle": "{œdataTypeœ:œOllamaEmbeddingsœ,œidœ:œOllamaEmbeddings-KdioMœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}",
                "target": "QdrantVectorStoreComponent-e7yEx",
                "targetHandle": "{œfieldNameœ:œembeddingœ,œidœ:œQdrantVectorStoreComponent-e7yExœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}",
                "data": {
                    "targetHandle": {
                        "fieldName": "embedding",
                        "id": "QdrantVectorStoreComponent-e7yEx",
                        "inputTypes": [
                            "Embeddings"
                        ],
                        "type": "other"
                    },
                    "sourceHandle": {
                        "dataType": "OllamaEmbeddings",
                        "id": "OllamaEmbeddings-KdioM",
                        "name": "embeddings",
                        "output_types": [
                            "Embeddings"
                        ]
                    }
                },
                "id": "reactflow__edge-OllamaEmbeddings-KdioM{œdataTypeœ:œOllamaEmbeddingsœ,œidœ:œOllamaEmbeddings-KdioMœ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}-QdrantVectorStoreComponent-e7yEx{œfieldNameœ:œembeddingœ,œidœ:œQdrantVectorStoreComponent-e7yExœ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}",
                "animated": false,
                "className": ""
            }
        ],
        "viewport": {
            "x": -33.152109610769685,
            "y": 27.92353844034119,
            "zoom": 0.42217487094973366
        }
    },
    "description": "Advanced RAG with Tavily search, using Gemma and ollama embeddings. Your data does not leave the machine",
    "name": "Advanced-RAG-ollama",
    "last_tested_version": "1.0.19.post2",
    "endpoint_name": null,
    "is_component": false
}